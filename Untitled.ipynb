{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Implementation of DDPG - Deep Deterministic Policy Gradient\n",
    "\n",
    "Algorithm and hyperparameter details can be found here: \n",
    "    http://arxiv.org/pdf/1509.02971v2.pdf\n",
    "\n",
    "The algorithm is tested on the Pendulum-v0 OpenAI gym task \n",
    "and developed with tflearn + Tensorflow\n",
    "\n",
    "Author: Patrick Emami\n",
    "\"\"\"\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "import logging\n",
    "\n",
    "# ===========================\n",
    "#   Actor and Critic DNNs\n",
    "# ===========================\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, continuous_act_space_flag, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.continuous_act_space_flag = continuous_act_space_flag\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 300)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        # If Actor acts on discrete action space, use Softmax\n",
    "        if self.continuous_act_space_flag is True:\n",
    "            out = tflearn.fully_connected(\n",
    "                net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        else:\n",
    "            out = tflearn.fully_connected(\n",
    "                net, self.a_dim, activation='softmax', weights_init=w_init)\n",
    "        # Scale output to -action_bound to action_bound\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        # TODO\n",
    "        powerful_critic = True\n",
    "        if powerful_critic is True:\n",
    "            net = tflearn.fully_connected(inputs, 400)\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "            net = tflearn.activations.relu(net)\n",
    "\n",
    "            # Add the action tensor in the 2nd hidden layer\n",
    "            # Use two temp layers to get the corresponding weights and biases\n",
    "            t1 = tflearn.fully_connected(net, 300)\n",
    "            t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "            net = tflearn.activation(\n",
    "                tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "        else:\n",
    "            net = tflearn.fully_connected(inputs, 400)\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "            net = tflearn.activations.relu(net)\n",
    "\n",
    "            # Add the action tensor in the 2nd hidden layer\n",
    "            # Use two temp layers to get the corresponding weights and biases\n",
    "            t1 = tflearn.fully_connected(net, 600)\n",
    "            t2 = tflearn.fully_connected(action, 600)\n",
    "\n",
    "            net = tflearn.activation(\n",
    "                tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "            # # Add the action tensor in the 2nd hidden layer\n",
    "            # # Use two temp layers to get the corresponding weights and biases\n",
    "            # t1 = tflearn.fully_connected(inputs, 300)\n",
    "            # t2 = tflearn.fully_connected(action, 300)\n",
    "            #\n",
    "            # net = tflearn.activation(\n",
    "            #     tf.matmul(inputs, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "        \n",
    "\n",
    "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "# ===========================\n",
    "#   Tensorflow Summary Ops\n",
    "# ===========================\n",
    "\n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "# ===========================\n",
    "#   Agent Training\n",
    "# ===========================\n",
    "\n",
    "def train(sess, env, args, actor, critic, actor_noise):\n",
    "\n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Epsilon parameter\n",
    "    epsilon = args['epsilon_max']\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n",
    "\n",
    "    # Time step\n",
    "    time_step = 0.\n",
    "\n",
    "    # Needed to enable BatchNorm. \n",
    "    # This hurts the performance on Pendulum but could be useful\n",
    "    # in other environments.\n",
    "    # tflearn.is_training(True)\n",
    "\n",
    "    for i in range(int(args['max_episodes'])):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        ep_steps = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            if args['render_env_flag']:\n",
    "                env.render()\n",
    "\n",
    "            # Added exploration noise\n",
    "            #a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i))\n",
    "            # TODO: different exploration strategy\n",
    "            a = []\n",
    "            action = []\n",
    "            exploration_strategy = args['exploration_strategy']\n",
    "            if exploration_strategy == 'action_noise':\n",
    "                a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n",
    "                # Convert continuous action into discrete action\n",
    "                if args['continuous_act_space_flag'] is True:\n",
    "                    action = a[0]\n",
    "                else:\n",
    "                    action = np.argmax(a[0])\n",
    "            elif exploration_strategy == 'epsilon_greedy':\n",
    "                if np.random.rand() < epsilon:\n",
    "                    if args['continuous_act_space_flag'] is True:\n",
    "                        a = np.reshape(env.action_space.sample(), (1, actor.a_dim))\n",
    "                    else:\n",
    "                        a = np.random.uniform(0, 1, (1, actor.a_dim))\n",
    "                else:\n",
    "                    a = actor.predict(np.reshape(s, (1, actor.s_dim)))\n",
    "                # Convert continuous action into discrete action\n",
    "                if args['continuous_act_space_flag'] is True:\n",
    "                    action = a[0]\n",
    "                else:\n",
    "                    action = np.argmax(a[0])\n",
    "            else:\n",
    "                print('Please choose a proper exploration strategy!')\n",
    "\n",
    "            s2, r, terminal, info = env.step(action)\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "\n",
    "            # Reduce epsilon\n",
    "            time_step += 1.\n",
    "            epsilon = args['epsilon_min'] + (args['epsilon_max'] - args['epsilon_min']) * np.exp(-args['epsilon_decay'] * time_step)\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > int(args['minibatch_size']):\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                    replay_buffer.sample_batch(int(args['minibatch_size']))\n",
    "\n",
    "                if args['double_ddpg_flag']:\n",
    "                    # Calculate targets: Double DDPG\n",
    "                    target_q = critic.predict_target(\n",
    "                            s2_batch, actor.predict(s2_batch))\n",
    "                else:\n",
    "                    # Calculate targets\n",
    "                    target_q = critic.predict_target(\n",
    "                            s2_batch, actor.predict_target(s2_batch))\n",
    "                \n",
    "                \n",
    "\n",
    "                y_i = []\n",
    "                for k in range(int(args['minibatch_size'])):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = critic.train(\n",
    "                    s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                a_outs = actor.predict(s_batch)\n",
    "                grads = critic.action_gradients(s_batch, a_outs)\n",
    "                actor.train(s_batch, grads[0])\n",
    "                \n",
    "                # Update target networks\n",
    "                if args['target_hard_copy_flag']:\n",
    "                    if ep_steps % args['target_hard_copy_interval'] == 0:\n",
    "                        actor.update_target_network()\n",
    "                        critic.update_target_network()\n",
    "                else:\n",
    "                    actor.update_target_network()\n",
    "                    critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "            ep_steps += 1\n",
    "\n",
    "            # if terminal or reach maximum length\n",
    "            if terminal:\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float((ep_steps + 1))\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, i)\n",
    "                writer.flush()\n",
    "\n",
    "                ep_stats = '| Episode: {0} | Steps: {1} | Reward: {2:.4f} | Qmax: {3:.4f}'.format(i,\n",
    "                                                                                             (ep_steps + 1),\n",
    "                                                                                             ep_reward,\n",
    "                                                                                             (ep_ave_max_q / float(ep_steps+1)))\n",
    "                print(ep_stats)\n",
    "                logging.info(ep_stats)\n",
    "                break\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        env = gym.make(args['env'])\n",
    "\n",
    "        np.random.seed(int(args['random_seed']))\n",
    "        tf.set_random_seed(int(args['random_seed']))\n",
    "        env.seed(int(args['random_seed']))\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        # Set action_dim for continuous and discrete action space\n",
    "        if args['continuous_act_space_flag'] is True:\n",
    "            action_dim = env.action_space.shape[0]\n",
    "            action_bound = env.action_space.high\n",
    "            # Ensure action bound is symmetric\n",
    "            assert (env.action_space.high == -env.action_space.low).all()\n",
    "        else:\n",
    "            action_dim = env.action_space.n\n",
    "            # If discrete action, actor uses Softmax and action_bound is always 1\n",
    "            action_bound = 1\n",
    "\n",
    "        # Use hardcopy way to update target NNs.\n",
    "        if args['target_hard_copy_flag'] is True:\n",
    "            args['tau'] = 1.0\n",
    "\n",
    "        actor = ActorNetwork(sess, args['continuous_act_space_flag'],\n",
    "                             state_dim, action_dim, action_bound,\n",
    "                             float(args['actor_lr']), float(args['tau']),\n",
    "                             int(args['minibatch_size']))\n",
    "\n",
    "        critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                               float(args['critic_lr']), float(args['tau']),\n",
    "                               float(args['gamma']),\n",
    "                               actor.get_num_trainable_vars())\n",
    "        \n",
    "        actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "        # Record videos\n",
    "        # Use the gym env Monitor wrapper\n",
    "        if args['use_gym_monitor_flag']:\n",
    "            monitor_dir = os.path.join(args['summary_dir'], 'gym_monitor')\n",
    "            env = wrappers.Monitor(env, monitor_dir,\n",
    "                                   resume=True,\n",
    "                                   video_callable=lambda count: count % args['record_video_every'] == 0)\n",
    "\n",
    "        train(sess, env, args, actor, critic, actor_noise)\n",
    "\n",
    "        if args['use_gym_monitor_flag']:\n",
    "            env.monitor.close()\n",
    "        else:\n",
    "            env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='provide arguments for DDPG agent')\n",
    "\n",
    "#     # agent parameters\n",
    "#     parser.add_argument('--actor-lr', type=float, default=0.0001, help='actor network learning rate')\n",
    "#     parser.add_argument('--critic-lr', type=float, default=0.001, help='critic network learning rate')\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99, help='discount factor for critic updates')\n",
    "#     parser.add_argument('--tau', type=float, default=0.001, help='soft target update parameter')\n",
    "#     parser.add_argument('--buffer-size', type=int, default=1000000, help='max size of the replay buffer')\n",
    "#     parser.add_argument('--minibatch-size', type=int, default=64, help='size of minibatch for minibatch-SGD')\n",
    "#     parser.add_argument(\"--continuous-act-space-flag\", action=\"store_true\", help='act on continuous action space')\n",
    "\n",
    "#     parser.add_argument(\"--exploration-strategy\", type=str, choices=[\"action_noise\", \"epsilon_greedy\"],\n",
    "#                         default='epsilon_greedy', help='action_noise or epsilon_greedy')\n",
    "#     parser.add_argument(\"--epsilon-max\", type=float, default=1.0, help='maximum of epsilon')\n",
    "#     parser.add_argument(\"--epsilon-min\", type=float, default=.01, help='minimum of epsilon')\n",
    "#     parser.add_argument(\"--epsilon-decay\", type=float, default=.001, help='epsilon decay')\n",
    "\n",
    "#     # train parameters\n",
    "#     parser.add_argument('--double-ddpg-flag', action=\"store_true\", help='True, if run double-ddpg-flag. Otherwise, False.')\n",
    "#     parser.add_argument('--target-hard-copy-flag', action=\"store_true\", help='Target network update method: hard copy')\n",
    "#     parser.add_argument('--target-hard-copy-interval', type=int, default=200, help='Target network update hard copy interval')\n",
    "\n",
    "#     # run parameters\n",
    "#     # HalfCheetah-v2, Ant-v2, InvertedPendulum-v2, Pendulum-v0\n",
    "#     parser.add_argument('--env', type=str, default='HalfCheetah-v2', help='choose the gym env- tested on {Pendulum-v0}')\n",
    "#     parser.add_argument('--random-seed', type=int, default=1234, help='random seed for repeatability')\n",
    "#     parser.add_argument('--max-episodes', type=int, default=50000, help='max num of episodes to do while training')\n",
    "#     # parser.add_argument(\"--max-episode-len\", type=int, default=1000, help='max length of 1 episode')\n",
    "#     parser.add_argument(\"--render-env-flag\", action=\"store_true\", help='render environment')\n",
    "#     parser.add_argument(\"--use-gym-monitor-flag\", action=\"store_true\", help='record gym results')\n",
    "#     parser.add_argument(\"--record-video-every\", type=int, default=1, help='record video every xx episodes')\n",
    "#     parser.add_argument(\"--monitor-dir\", type=str, default='./results/gym_ddpg', help='directory for storing gym results')\n",
    "#     parser.add_argument(\"--summary-dir\", type=str, default='./results/tf_ddpg/HalfCheetah-v2/ddpg_Tau_0.001_run1', help='directory for storing tensorboard info')\n",
    "\n",
    "\n",
    "#     parser.set_defaults(use_gym_monitor=False)\n",
    "\n",
    "#     # args = vars(parser.parse_args())\n",
    "#     # args = parser.parse_args()\n",
    "#     args = vars(parser.parse_args())\n",
    "\n",
    "#     pp.pprint(args)\n",
    "\n",
    "#     if not os.path.exists(args['summary_dir']):\n",
    "#         os.makedirs(args['summary_dir'])\n",
    "#     log_dir = os.path.join(args['summary_dir'], 'ddpg_running_log.log')\n",
    "#     logging.basicConfig(filename=log_dir, filemode='a', level=logging.INFO)\n",
    "#     for key in args.keys():\n",
    "#         logging.info('{0}: {1}'.format(key, args[key]))\n",
    "\n",
    "#     main(args)\n",
    "\n",
    "#     # python ddpg_discrete_action.py --env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Qmax Value is illegal; using Qmax_Value instead.\n",
      "WARNING:tensorflow:Issue encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "| Episode: 0 | Steps: 1601 | Reward: -61.3659 | Qmax: 0.1831\n",
      "| Episode: 1 | Steps: 84 | Reward: -107.5455 | Qmax: 0.2424\n",
      "| Episode: 2 | Steps: 53 | Reward: -111.7408 | Qmax: 0.1831\n",
      "| Episode: 3 | Steps: 59 | Reward: -117.5770 | Qmax: 0.0241\n",
      "| Episode: 4 | Steps: 140 | Reward: -120.7175 | Qmax: 0.0428\n",
      "| Episode: 5 | Steps: 547 | Reward: -166.9157 | Qmax: 0.1489\n",
      "| Episode: 6 | Steps: 199 | Reward: -123.7205 | Qmax: 0.1530\n",
      "| Episode: 7 | Steps: 106 | Reward: -113.0892 | Qmax: 0.1617\n",
      "| Episode: 8 | Steps: 41 | Reward: -118.9910 | Qmax: 0.1635\n",
      "| Episode: 9 | Steps: 97 | Reward: -112.9919 | Qmax: 0.2022\n",
      "| Episode: 10 | Steps: 113 | Reward: -117.0318 | Qmax: 0.1746\n",
      "| Episode: 11 | Steps: 41 | Reward: -118.8450 | Qmax: 0.1705\n",
      "| Episode: 12 | Steps: 135 | Reward: -118.2076 | Qmax: 0.1285\n",
      "| Episode: 13 | Steps: 70 | Reward: -112.8233 | Qmax: 0.2131\n",
      "| Episode: 14 | Steps: 153 | Reward: -132.5064 | Qmax: 0.2884\n",
      "| Episode: 15 | Steps: 170 | Reward: -122.7484 | Qmax: 0.2274\n",
      "| Episode: 16 | Steps: 104 | Reward: -129.2159 | Qmax: 0.2131\n",
      "| Episode: 17 | Steps: 153 | Reward: -121.0676 | Qmax: 0.1481\n",
      "| Episode: 18 | Steps: 128 | Reward: -134.1151 | Qmax: 0.1838\n",
      "| Episode: 19 | Steps: 78 | Reward: -123.6095 | Qmax: 0.1411\n",
      "| Episode: 20 | Steps: 106 | Reward: -130.4156 | Qmax: 0.0943\n",
      "| Episode: 21 | Steps: 145 | Reward: -136.7208 | Qmax: 0.0785\n",
      "| Episode: 22 | Steps: 146 | Reward: -135.9666 | Qmax: 0.0671\n",
      "| Episode: 23 | Steps: 170 | Reward: -137.4852 | Qmax: 0.1349\n",
      "| Episode: 24 | Steps: 179 | Reward: -123.2209 | Qmax: 0.1444\n",
      "| Episode: 25 | Steps: 101 | Reward: -128.3090 | Qmax: 0.0886\n",
      "| Episode: 26 | Steps: 107 | Reward: -130.0135 | Qmax: 0.0902\n",
      "| Episode: 27 | Steps: 166 | Reward: -138.6837 | Qmax: 0.1674\n",
      "| Episode: 28 | Steps: 143 | Reward: -134.9488 | Qmax: 0.1422\n",
      "| Episode: 29 | Steps: 1255 | Reward: -256.7206 | Qmax: 0.2662\n",
      "| Episode: 30 | Steps: 61 | Reward: -103.3055 | Qmax: 0.3155\n",
      "| Episode: 31 | Steps: 62 | Reward: -103.4333 | Qmax: 0.3460\n",
      "| Episode: 32 | Steps: 62 | Reward: -103.3800 | Qmax: 0.3932\n",
      "| Episode: 33 | Steps: 60 | Reward: -103.8318 | Qmax: 0.4732\n",
      "| Episode: 34 | Steps: 61 | Reward: -103.3126 | Qmax: 0.4553\n",
      "| Episode: 35 | Steps: 62 | Reward: -103.2864 | Qmax: 0.5208\n",
      "| Episode: 36 | Steps: 62 | Reward: -103.4334 | Qmax: 0.5319\n",
      "| Episode: 37 | Steps: 61 | Reward: -103.3050 | Qmax: 0.5613\n",
      "| Episode: 38 | Steps: 66 | Reward: -102.4900 | Qmax: 0.6040\n",
      "| Episode: 39 | Steps: 62 | Reward: -103.3356 | Qmax: 0.7450\n",
      "| Episode: 40 | Steps: 66 | Reward: -102.5618 | Qmax: 0.9527\n",
      "| Episode: 41 | Steps: 53 | Reward: -102.6990 | Qmax: 0.8416\n",
      "| Episode: 42 | Steps: 48 | Reward: -105.3440 | Qmax: 0.7591\n",
      "| Episode: 43 | Steps: 62 | Reward: -103.4310 | Qmax: 0.6966\n",
      "| Episode: 44 | Steps: 57 | Reward: -104.9596 | Qmax: 0.7332\n",
      "| Episode: 45 | Steps: 46 | Reward: -106.8506 | Qmax: 0.8458\n",
      "| Episode: 46 | Steps: 62 | Reward: -103.3464 | Qmax: 0.7524\n",
      "| Episode: 47 | Steps: 61 | Reward: -103.2584 | Qmax: 0.8786\n",
      "| Episode: 48 | Steps: 62 | Reward: -103.3731 | Qmax: 0.8289\n",
      "| Episode: 49 | Steps: 62 | Reward: -103.3618 | Qmax: 0.8320\n",
      "| Episode: 50 | Steps: 48 | Reward: -106.9622 | Qmax: 0.7718\n",
      "| Episode: 51 | Steps: 61 | Reward: -103.2833 | Qmax: 1.1240\n",
      "| Episode: 52 | Steps: 46 | Reward: -106.7732 | Qmax: 1.0225\n",
      "| Episode: 53 | Steps: 47 | Reward: -106.7002 | Qmax: 0.9248\n",
      "| Episode: 54 | Steps: 66 | Reward: -102.6051 | Qmax: 0.8656\n",
      "| Episode: 55 | Steps: 61 | Reward: -103.2281 | Qmax: 1.0574\n",
      "| Episode: 56 | Steps: 66 | Reward: -102.6416 | Qmax: 0.9793\n",
      "| Episode: 57 | Steps: 62 | Reward: -103.1055 | Qmax: 1.1607\n",
      "| Episode: 58 | Steps: 49 | Reward: -103.5414 | Qmax: 1.5275\n",
      "| Episode: 59 | Steps: 52 | Reward: -102.0022 | Qmax: 1.6562\n",
      "| Episode: 60 | Steps: 51 | Reward: -102.2163 | Qmax: 1.4514\n",
      "| Episode: 61 | Steps: 51 | Reward: -102.5566 | Qmax: 1.5441\n",
      "| Episode: 62 | Steps: 51 | Reward: -102.3601 | Qmax: 1.8278\n",
      "| Episode: 63 | Steps: 51 | Reward: -102.5570 | Qmax: 1.7363\n",
      "| Episode: 64 | Steps: 51 | Reward: -102.7161 | Qmax: 1.6527\n",
      "| Episode: 65 | Steps: 51 | Reward: -102.3482 | Qmax: 1.7781\n",
      "| Episode: 66 | Steps: 52 | Reward: -102.1833 | Qmax: 1.8526\n",
      "| Episode: 67 | Steps: 51 | Reward: -103.4825 | Qmax: 2.0550\n",
      "| Episode: 68 | Steps: 51 | Reward: -102.3903 | Qmax: 1.7764\n",
      "| Episode: 69 | Steps: 51 | Reward: -102.5163 | Qmax: 1.9871\n",
      "| Episode: 70 | Steps: 52 | Reward: -102.6185 | Qmax: 2.1003\n",
      "| Episode: 71 | Steps: 51 | Reward: -102.2898 | Qmax: 2.1295\n",
      "| Episode: 72 | Steps: 50 | Reward: -102.6293 | Qmax: 1.9595\n",
      "| Episode: 73 | Steps: 49 | Reward: -106.9843 | Qmax: 2.3397\n",
      "| Episode: 74 | Steps: 65 | Reward: -106.7620 | Qmax: 2.1652\n",
      "| Episode: 75 | Steps: 49 | Reward: -126.1920 | Qmax: 2.3391\n",
      "| Episode: 76 | Steps: 49 | Reward: -125.0608 | Qmax: 2.2606\n",
      "| Episode: 77 | Steps: 48 | Reward: -126.9662 | Qmax: 2.1534\n",
      "| Episode: 78 | Steps: 48 | Reward: -126.7064 | Qmax: 2.3955\n",
      "| Episode: 79 | Steps: 48 | Reward: -126.9830 | Qmax: 2.2359\n",
      "| Episode: 80 | Steps: 48 | Reward: -126.8513 | Qmax: 2.2634\n",
      "| Episode: 81 | Steps: 48 | Reward: -126.7607 | Qmax: 2.4708\n",
      "| Episode: 82 | Steps: 48 | Reward: -126.6895 | Qmax: 2.1743\n",
      "| Episode: 83 | Steps: 48 | Reward: -126.7935 | Qmax: 2.3966\n",
      "| Episode: 84 | Steps: 48 | Reward: -127.1024 | Qmax: 2.4086\n",
      "| Episode: 85 | Steps: 48 | Reward: -127.0519 | Qmax: 2.3662\n",
      "| Episode: 86 | Steps: 50 | Reward: -121.9855 | Qmax: 2.3187\n",
      "| Episode: 87 | Steps: 48 | Reward: -127.1020 | Qmax: 2.4319\n",
      "| Episode: 88 | Steps: 48 | Reward: -127.0014 | Qmax: 2.4965\n",
      "| Episode: 89 | Steps: 48 | Reward: -127.0369 | Qmax: 2.4939\n",
      "| Episode: 90 | Steps: 48 | Reward: -127.1693 | Qmax: 2.5011\n",
      "| Episode: 91 | Steps: 48 | Reward: -127.0672 | Qmax: 2.6264\n",
      "| Episode: 92 | Steps: 48 | Reward: -126.9840 | Qmax: 2.6160\n",
      "| Episode: 93 | Steps: 48 | Reward: -127.1860 | Qmax: 2.5984\n",
      "| Episode: 94 | Steps: 50 | Reward: -125.6790 | Qmax: 2.6434\n",
      "| Episode: 95 | Steps: 48 | Reward: -127.1714 | Qmax: 2.5592\n",
      "| Episode: 96 | Steps: 48 | Reward: -126.9846 | Qmax: 2.5960\n",
      "| Episode: 97 | Steps: 49 | Reward: -125.7957 | Qmax: 2.7273\n",
      "| Episode: 98 | Steps: 48 | Reward: -126.9833 | Qmax: 2.7619\n",
      "| Episode: 99 | Steps: 48 | Reward: -127.1055 | Qmax: 2.7955\n",
      "| Episode: 100 | Steps: 48 | Reward: -127.0687 | Qmax: 2.7153\n",
      "| Episode: 101 | Steps: 49 | Reward: -126.2081 | Qmax: 2.7966\n",
      "| Episode: 102 | Steps: 48 | Reward: -127.0819 | Qmax: 2.8829\n",
      "| Episode: 103 | Steps: 48 | Reward: -127.2728 | Qmax: 2.7096\n",
      "| Episode: 104 | Steps: 48 | Reward: -127.2475 | Qmax: 2.9214\n",
      "| Episode: 105 | Steps: 48 | Reward: -127.1990 | Qmax: 2.9079\n",
      "| Episode: 106 | Steps: 47 | Reward: -119.7876 | Qmax: 2.8618\n",
      "| Episode: 107 | Steps: 48 | Reward: -127.2357 | Qmax: 2.7680\n",
      "| Episode: 108 | Steps: 48 | Reward: -127.1359 | Qmax: 2.9333\n",
      "| Episode: 109 | Steps: 50 | Reward: -128.8732 | Qmax: 2.9661\n",
      "| Episode: 110 | Steps: 49 | Reward: -128.1777 | Qmax: 2.8786\n",
      "| Episode: 111 | Steps: 48 | Reward: -127.1844 | Qmax: 3.0115\n",
      "| Episode: 112 | Steps: 48 | Reward: -127.2230 | Qmax: 2.9684\n",
      "| Episode: 113 | Steps: 48 | Reward: -127.2233 | Qmax: 2.9640\n",
      "| Episode: 114 | Steps: 48 | Reward: -127.0095 | Qmax: 3.0319\n",
      "| Episode: 115 | Steps: 48 | Reward: -127.1827 | Qmax: 3.1613\n",
      "| Episode: 116 | Steps: 48 | Reward: -127.2752 | Qmax: 2.8690\n",
      "| Episode: 117 | Steps: 48 | Reward: -127.2338 | Qmax: 3.0171\n",
      "| Episode: 118 | Steps: 48 | Reward: -127.1554 | Qmax: 2.9965\n",
      "| Episode: 119 | Steps: 48 | Reward: -127.1671 | Qmax: 3.0422\n",
      "| Episode: 120 | Steps: 48 | Reward: -127.1948 | Qmax: 3.1150\n",
      "| Episode: 121 | Steps: 48 | Reward: -126.9400 | Qmax: 2.9207\n",
      "| Episode: 122 | Steps: 48 | Reward: -126.9729 | Qmax: 2.9830\n",
      "| Episode: 123 | Steps: 48 | Reward: -126.9447 | Qmax: 3.0522\n",
      "| Episode: 124 | Steps: 48 | Reward: -127.0391 | Qmax: 3.1317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 125 | Steps: 48 | Reward: -127.1699 | Qmax: 3.0386\n",
      "| Episode: 126 | Steps: 48 | Reward: -127.2324 | Qmax: 3.0804\n",
      "| Episode: 127 | Steps: 48 | Reward: -127.0714 | Qmax: 3.1071\n",
      "| Episode: 128 | Steps: 49 | Reward: -128.3151 | Qmax: 3.2951\n",
      "| Episode: 129 | Steps: 48 | Reward: -127.1589 | Qmax: 3.1971\n",
      "| Episode: 130 | Steps: 51 | Reward: -121.9975 | Qmax: 3.0900\n",
      "| Episode: 131 | Steps: 48 | Reward: -127.2298 | Qmax: 3.1347\n",
      "| Episode: 132 | Steps: 49 | Reward: -125.9125 | Qmax: 3.1715\n",
      "| Episode: 133 | Steps: 48 | Reward: -127.1892 | Qmax: 3.1985\n",
      "| Episode: 134 | Steps: 48 | Reward: -127.2465 | Qmax: 3.1843\n",
      "| Episode: 135 | Steps: 48 | Reward: -125.8803 | Qmax: 3.2772\n",
      "| Episode: 136 | Steps: 48 | Reward: -127.2184 | Qmax: 3.0941\n",
      "| Episode: 137 | Steps: 48 | Reward: -127.0005 | Qmax: 3.2879\n",
      "| Episode: 138 | Steps: 48 | Reward: -127.0988 | Qmax: 3.3440\n",
      "| Episode: 139 | Steps: 50 | Reward: -125.1853 | Qmax: 3.2700\n",
      "| Episode: 140 | Steps: 48 | Reward: -126.8792 | Qmax: 3.0141\n",
      "| Episode: 141 | Steps: 48 | Reward: -127.0009 | Qmax: 3.0836\n",
      "| Episode: 142 | Steps: 49 | Reward: -126.2168 | Qmax: 3.2181\n",
      "| Episode: 143 | Steps: 48 | Reward: -126.9211 | Qmax: 3.1455\n",
      "| Episode: 144 | Steps: 48 | Reward: -126.8754 | Qmax: 3.1667\n",
      "| Episode: 145 | Steps: 48 | Reward: -127.0118 | Qmax: 3.2874\n",
      "| Episode: 146 | Steps: 48 | Reward: -126.9864 | Qmax: 3.2676\n",
      "| Episode: 147 | Steps: 46 | Reward: -118.5620 | Qmax: 3.3270\n",
      "| Episode: 148 | Steps: 48 | Reward: -126.8402 | Qmax: 3.2933\n",
      "| Episode: 149 | Steps: 49 | Reward: -127.2535 | Qmax: 3.3033\n",
      "| Episode: 150 | Steps: 49 | Reward: -126.9742 | Qmax: 3.1234\n",
      "| Episode: 151 | Steps: 51 | Reward: -123.7642 | Qmax: 3.2423\n",
      "| Episode: 152 | Steps: 51 | Reward: -102.1241 | Qmax: 3.1934\n",
      "| Episode: 153 | Steps: 51 | Reward: -102.2471 | Qmax: 3.3652\n",
      "| Episode: 154 | Steps: 51 | Reward: -102.2172 | Qmax: 3.2675\n",
      "| Episode: 155 | Steps: 51 | Reward: -102.0345 | Qmax: 3.2340\n",
      "| Episode: 156 | Steps: 51 | Reward: -102.1989 | Qmax: 3.2064\n",
      "| Episode: 157 | Steps: 51 | Reward: -102.0920 | Qmax: 3.3723\n",
      "| Episode: 158 | Steps: 51 | Reward: -102.1162 | Qmax: 3.3900\n",
      "| Episode: 159 | Steps: 50 | Reward: -102.6192 | Qmax: 3.1552\n",
      "| Episode: 160 | Steps: 52 | Reward: -102.1060 | Qmax: 3.3423\n",
      "| Episode: 161 | Steps: 51 | Reward: -102.1316 | Qmax: 3.3262\n",
      "| Episode: 162 | Steps: 51 | Reward: -102.0685 | Qmax: 3.4198\n",
      "| Episode: 163 | Steps: 51 | Reward: -102.1671 | Qmax: 3.4341\n",
      "| Episode: 164 | Steps: 51 | Reward: -102.1360 | Qmax: 3.2912\n",
      "| Episode: 165 | Steps: 51 | Reward: -102.1027 | Qmax: 3.3987\n",
      "| Episode: 166 | Steps: 51 | Reward: -103.2723 | Qmax: 3.3637\n",
      "| Episode: 167 | Steps: 51 | Reward: -101.9998 | Qmax: 3.3690\n",
      "| Episode: 168 | Steps: 49 | Reward: -103.3233 | Qmax: 3.3684\n",
      "| Episode: 169 | Steps: 51 | Reward: -102.1400 | Qmax: 3.4534\n",
      "| Episode: 170 | Steps: 50 | Reward: -102.9158 | Qmax: 3.3677\n",
      "| Episode: 171 | Steps: 49 | Reward: -103.4443 | Qmax: 3.2953\n",
      "| Episode: 172 | Steps: 52 | Reward: -101.8408 | Qmax: 3.3926\n",
      "| Episode: 173 | Steps: 51 | Reward: -102.0335 | Qmax: 3.4873\n",
      "| Episode: 174 | Steps: 51 | Reward: -102.1137 | Qmax: 3.3488\n",
      "| Episode: 175 | Steps: 51 | Reward: -102.1246 | Qmax: 3.4571\n",
      "| Episode: 176 | Steps: 49 | Reward: -103.5416 | Qmax: 3.4748\n",
      "| Episode: 177 | Steps: 51 | Reward: -102.1199 | Qmax: 3.4121\n",
      "| Episode: 178 | Steps: 51 | Reward: -102.1850 | Qmax: 3.4128\n",
      "| Episode: 179 | Steps: 51 | Reward: -102.0894 | Qmax: 3.3381\n",
      "| Episode: 180 | Steps: 51 | Reward: -102.1010 | Qmax: 3.3812\n",
      "| Episode: 181 | Steps: 49 | Reward: -103.0758 | Qmax: 3.4967\n",
      "| Episode: 182 | Steps: 52 | Reward: -101.9629 | Qmax: 3.3628\n",
      "| Episode: 183 | Steps: 51 | Reward: -102.1271 | Qmax: 3.3783\n",
      "| Episode: 184 | Steps: 49 | Reward: -103.5390 | Qmax: 3.3251\n",
      "| Episode: 185 | Steps: 51 | Reward: -102.1129 | Qmax: 3.3441\n",
      "| Episode: 186 | Steps: 47 | Reward: -104.1348 | Qmax: 3.5214\n",
      "| Episode: 187 | Steps: 51 | Reward: -102.1129 | Qmax: 3.4372\n",
      "| Episode: 188 | Steps: 51 | Reward: -102.0897 | Qmax: 3.3758\n",
      "| Episode: 189 | Steps: 51 | Reward: -102.0689 | Qmax: 3.4038\n",
      "| Episode: 190 | Steps: 51 | Reward: -102.0844 | Qmax: 3.3401\n",
      "| Episode: 191 | Steps: 49 | Reward: -103.4890 | Qmax: 3.4073\n",
      "| Episode: 192 | Steps: 51 | Reward: -102.1697 | Qmax: 3.4425\n",
      "| Episode: 193 | Steps: 49 | Reward: -103.2805 | Qmax: 3.4488\n",
      "| Episode: 194 | Steps: 51 | Reward: -102.1218 | Qmax: 3.4167\n",
      "| Episode: 195 | Steps: 51 | Reward: -102.0688 | Qmax: 3.2835\n",
      "| Episode: 196 | Steps: 51 | Reward: -102.1261 | Qmax: 3.4564\n",
      "| Episode: 197 | Steps: 51 | Reward: -102.1462 | Qmax: 3.4859\n",
      "| Episode: 198 | Steps: 51 | Reward: -102.1279 | Qmax: 3.5016\n",
      "| Episode: 199 | Steps: 51 | Reward: -102.0997 | Qmax: 3.4663\n",
      "| Episode: 200 | Steps: 51 | Reward: -102.1178 | Qmax: 3.3778\n",
      "| Episode: 201 | Steps: 51 | Reward: -102.1829 | Qmax: 3.4044\n",
      "| Episode: 202 | Steps: 56 | Reward: -101.4933 | Qmax: 3.3508\n",
      "| Episode: 203 | Steps: 51 | Reward: -102.2081 | Qmax: 3.3826\n",
      "| Episode: 204 | Steps: 51 | Reward: -102.5465 | Qmax: 3.4298\n",
      "| Episode: 205 | Steps: 51 | Reward: -102.5217 | Qmax: 3.3345\n",
      "| Episode: 206 | Steps: 51 | Reward: -102.5362 | Qmax: 3.3726\n",
      "| Episode: 207 | Steps: 47 | Reward: -127.5038 | Qmax: 3.3958\n",
      "| Episode: 208 | Steps: 46 | Reward: -127.4668 | Qmax: 3.3897\n",
      "| Episode: 209 | Steps: 46 | Reward: -127.4967 | Qmax: 3.4995\n",
      "| Episode: 210 | Steps: 46 | Reward: -127.4258 | Qmax: 3.3407\n",
      "| Episode: 211 | Steps: 47 | Reward: -128.0767 | Qmax: 3.3674\n",
      "| Episode: 212 | Steps: 46 | Reward: -127.3068 | Qmax: 3.4642\n",
      "| Episode: 213 | Steps: 1601 | Reward: -181.7011 | Qmax: 3.4179\n",
      "| Episode: 214 | Steps: 46 | Reward: -127.4158 | Qmax: 3.3300\n",
      "| Episode: 215 | Steps: 49 | Reward: -121.8322 | Qmax: 3.1656\n",
      "| Episode: 216 | Steps: 46 | Reward: -127.4618 | Qmax: 3.2707\n",
      "| Episode: 217 | Steps: 46 | Reward: -127.4715 | Qmax: 3.3730\n",
      "| Episode: 218 | Steps: 46 | Reward: -127.4269 | Qmax: 3.2156\n",
      "| Episode: 219 | Steps: 46 | Reward: -127.4706 | Qmax: 3.2172\n",
      "| Episode: 220 | Steps: 47 | Reward: -128.0086 | Qmax: 3.1737\n",
      "| Episode: 221 | Steps: 47 | Reward: -127.9851 | Qmax: 3.2190\n",
      "| Episode: 222 | Steps: 47 | Reward: -128.0353 | Qmax: 3.1891\n",
      "| Episode: 223 | Steps: 46 | Reward: -127.5066 | Qmax: 3.2547\n",
      "| Episode: 224 | Steps: 46 | Reward: -127.4654 | Qmax: 3.1737\n",
      "| Episode: 225 | Steps: 49 | Reward: -124.8781 | Qmax: 3.1871\n",
      "| Episode: 226 | Steps: 51 | Reward: -131.6578 | Qmax: 3.2350\n",
      "| Episode: 227 | Steps: 46 | Reward: -127.5056 | Qmax: 3.0838\n",
      "| Episode: 228 | Steps: 46 | Reward: -127.5224 | Qmax: 3.0157\n",
      "| Episode: 229 | Steps: 47 | Reward: -128.0680 | Qmax: 3.0542\n",
      "| Episode: 230 | Steps: 47 | Reward: -128.0354 | Qmax: 3.0970\n",
      "| Episode: 231 | Steps: 47 | Reward: -128.0149 | Qmax: 3.2000\n",
      "| Episode: 232 | Steps: 47 | Reward: -127.9176 | Qmax: 3.0735\n",
      "| Episode: 233 | Steps: 47 | Reward: -127.5786 | Qmax: 3.0476\n",
      "| Episode: 234 | Steps: 47 | Reward: -127.2992 | Qmax: 2.9659\n",
      "| Episode: 235 | Steps: 48 | Reward: -126.8438 | Qmax: 3.0859\n",
      "| Episode: 236 | Steps: 102 | Reward: -108.3190 | Qmax: 3.0258\n",
      "| Episode: 237 | Steps: 110 | Reward: -103.3211 | Qmax: 3.0838\n",
      "| Episode: 238 | Steps: 85 | Reward: -100.9025 | Qmax: 3.1230\n",
      "| Episode: 239 | Steps: 60 | Reward: -102.9357 | Qmax: 3.0635\n",
      "| Episode: 240 | Steps: 68 | Reward: -101.9862 | Qmax: 3.1073\n",
      "| Episode: 241 | Steps: 54 | Reward: -101.0669 | Qmax: 3.1347\n",
      "| Episode: 242 | Steps: 55 | Reward: -101.2003 | Qmax: 3.1731\n",
      "| Episode: 243 | Steps: 54 | Reward: -101.3539 | Qmax: 3.2335\n",
      "| Episode: 244 | Steps: 49 | Reward: -103.1769 | Qmax: 3.2264\n",
      "| Episode: 245 | Steps: 48 | Reward: -103.1494 | Qmax: 3.1543\n",
      "| Episode: 246 | Steps: 45 | Reward: -104.5281 | Qmax: 3.2802\n",
      "| Episode: 247 | Steps: 48 | Reward: -102.5511 | Qmax: 3.2022\n",
      "| Episode: 248 | Steps: 60 | Reward: -103.6354 | Qmax: 3.1851\n",
      "| Episode: 249 | Steps: 50 | Reward: -105.7729 | Qmax: 3.2098\n",
      "| Episode: 250 | Steps: 82 | Reward: -102.0132 | Qmax: 3.2638\n",
      "| Episode: 251 | Steps: 78 | Reward: -102.5945 | Qmax: 3.2023\n",
      "| Episode: 252 | Steps: 74 | Reward: -105.4584 | Qmax: 3.3257\n",
      "| Episode: 253 | Steps: 138 | Reward: -140.1457 | Qmax: 3.2888\n",
      "| Episode: 254 | Steps: 68 | Reward: -124.7630 | Qmax: 3.2457\n",
      "| Episode: 255 | Steps: 106 | Reward: -104.6812 | Qmax: 3.2978\n",
      "| Episode: 256 | Steps: 106 | Reward: -104.6295 | Qmax: 3.3452\n",
      "| Episode: 257 | Steps: 107 | Reward: -104.5499 | Qmax: 3.3530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 258 | Steps: 106 | Reward: -104.5850 | Qmax: 3.4860\n",
      "| Episode: 259 | Steps: 104 | Reward: -104.4516 | Qmax: 3.5015\n",
      "| Episode: 260 | Steps: 93 | Reward: -105.3873 | Qmax: 3.5776\n",
      "| Episode: 261 | Steps: 48 | Reward: -108.9054 | Qmax: 3.5299\n",
      "| Episode: 262 | Steps: 106 | Reward: -104.6938 | Qmax: 3.6070\n",
      "| Episode: 263 | Steps: 108 | Reward: -104.6331 | Qmax: 3.6413\n",
      "| Episode: 264 | Steps: 107 | Reward: -104.7186 | Qmax: 3.7827\n",
      "| Episode: 265 | Steps: 105 | Reward: -111.7458 | Qmax: 3.7964\n",
      "| Episode: 266 | Steps: 1601 | Reward: -177.5198 | Qmax: 3.9256\n",
      "| Episode: 267 | Steps: 1208 | Reward: -247.1201 | Qmax: 3.9883\n",
      "| Episode: 268 | Steps: 44 | Reward: -107.6092 | Qmax: 3.9302\n",
      "| Episode: 269 | Steps: 76 | Reward: -105.7638 | Qmax: 3.8973\n",
      "| Episode: 270 | Steps: 47 | Reward: -106.2266 | Qmax: 3.6925\n",
      "| Episode: 271 | Steps: 106 | Reward: -134.9182 | Qmax: 4.0328\n",
      "| Episode: 272 | Steps: 67 | Reward: -104.5017 | Qmax: 3.9423\n",
      "| Episode: 273 | Steps: 69 | Reward: -124.4838 | Qmax: 3.9910\n",
      "| Episode: 274 | Steps: 48 | Reward: -107.5778 | Qmax: 3.9409\n",
      "| Episode: 275 | Steps: 44 | Reward: -107.1525 | Qmax: 3.9615\n",
      "| Episode: 276 | Steps: 65 | Reward: -105.3181 | Qmax: 4.0700\n",
      "| Episode: 277 | Steps: 68 | Reward: -104.5713 | Qmax: 3.9073\n",
      "| Episode: 278 | Steps: 65 | Reward: -105.4600 | Qmax: 4.0588\n",
      "| Episode: 279 | Steps: 46 | Reward: -108.9683 | Qmax: 3.9615\n",
      "| Episode: 280 | Steps: 67 | Reward: -103.8444 | Qmax: 3.8444\n",
      "| Episode: 281 | Steps: 59 | Reward: -102.9191 | Qmax: 4.1455\n",
      "| Episode: 282 | Steps: 54 | Reward: -102.7365 | Qmax: 4.1263\n",
      "| Episode: 283 | Steps: 53 | Reward: -103.2547 | Qmax: 3.9674\n",
      "| Episode: 284 | Steps: 54 | Reward: -102.3303 | Qmax: 4.1557\n",
      "| Episode: 285 | Steps: 54 | Reward: -102.8138 | Qmax: 4.2182\n",
      "| Episode: 286 | Steps: 52 | Reward: -104.3103 | Qmax: 4.0316\n",
      "| Episode: 287 | Steps: 56 | Reward: -102.6595 | Qmax: 4.1285\n",
      "| Episode: 288 | Steps: 52 | Reward: -102.9728 | Qmax: 4.0504\n",
      "| Episode: 289 | Steps: 98 | Reward: -104.9330 | Qmax: 4.1585\n",
      "| Episode: 290 | Steps: 100 | Reward: -105.4027 | Qmax: 4.0963\n",
      "| Episode: 291 | Steps: 103 | Reward: -104.7392 | Qmax: 4.1856\n",
      "| Episode: 292 | Steps: 49 | Reward: -108.1864 | Qmax: 4.0474\n",
      "| Episode: 293 | Steps: 106 | Reward: -104.8684 | Qmax: 4.1489\n",
      "| Episode: 294 | Steps: 92 | Reward: -103.7590 | Qmax: 4.1253\n",
      "| Episode: 295 | Steps: 106 | Reward: -104.8302 | Qmax: 4.2016\n",
      "| Episode: 296 | Steps: 106 | Reward: -104.8566 | Qmax: 4.1550\n",
      "| Episode: 297 | Steps: 102 | Reward: -105.5718 | Qmax: 4.3143\n",
      "| Episode: 298 | Steps: 105 | Reward: -104.6595 | Qmax: 4.2439\n",
      "| Episode: 299 | Steps: 72 | Reward: -103.4298 | Qmax: 4.2294\n",
      "| Episode: 300 | Steps: 101 | Reward: -104.8135 | Qmax: 4.2903\n",
      "| Episode: 301 | Steps: 101 | Reward: -104.6251 | Qmax: 4.3019\n",
      "| Episode: 302 | Steps: 64 | Reward: -108.4473 | Qmax: 4.3616\n",
      "| Episode: 303 | Steps: 49 | Reward: -121.3892 | Qmax: 4.3639\n",
      "| Episode: 304 | Steps: 52 | Reward: -122.6315 | Qmax: 4.2973\n",
      "| Episode: 305 | Steps: 74 | Reward: -130.7960 | Qmax: 4.2908\n",
      "| Episode: 306 | Steps: 1601 | Reward: -156.7439 | Qmax: 4.4876\n",
      "| Episode: 307 | Steps: 1601 | Reward: -160.7426 | Qmax: 4.7924\n",
      "| Episode: 308 | Steps: 1601 | Reward: -176.9639 | Qmax: 5.0941\n",
      "| Episode: 309 | Steps: 1601 | Reward: -177.5862 | Qmax: 5.3986\n",
      "| Episode: 310 | Steps: 1601 | Reward: -179.0717 | Qmax: 5.7856\n",
      "| Episode: 311 | Steps: 1601 | Reward: -180.0398 | Qmax: 6.3878\n",
      "| Episode: 312 | Steps: 1601 | Reward: -175.3407 | Qmax: 7.1347\n",
      "| Episode: 313 | Steps: 1601 | Reward: -156.1210 | Qmax: 7.8738\n",
      "| Episode: 314 | Steps: 1601 | Reward: -170.3795 | Qmax: 8.7261\n",
      "| Episode: 315 | Steps: 1601 | Reward: -177.1893 | Qmax: 9.4527\n",
      "| Episode: 316 | Steps: 1601 | Reward: -177.9913 | Qmax: 10.2439\n",
      "| Episode: 317 | Steps: 1601 | Reward: -177.7780 | Qmax: 10.8257\n",
      "| Episode: 318 | Steps: 1601 | Reward: -150.2366 | Qmax: 11.3684\n",
      "| Episode: 319 | Steps: 1601 | Reward: -116.5524 | Qmax: 11.7782\n",
      "| Episode: 320 | Steps: 1601 | Reward: -116.4450 | Qmax: 12.4270\n"
     ]
    }
   ],
   "source": [
    "args = {'actor_lr': 0.0001,\n",
    "        'buffer_size': 1000000,\n",
    "        'continuous_act_space_flag': True,\n",
    "        'critic_lr': 0.001,\n",
    "        'double_ddpg_flag': True,\n",
    "        'env': 'BipedalWalker-v2',\n",
    "        'epsilon_decay': 0.001,\n",
    "        'epsilon_max': 1.0,\n",
    "        'epsilon_min': 0.01,\n",
    "        'exploration_strategy': 'epsilon_greedy',\n",
    "        'gamma': 0.99,\n",
    "        'max_episodes': 50000,\n",
    "        'minibatch_size': 64,\n",
    "        'monitor_dir': './results/gym_ddpg',\n",
    "        'random_seed': 1234,\n",
    "        'record_video_every': 1,\n",
    "        'render_env_flag': False,\n",
    "        'summary_dir': './results/BipedalWalker-v2/double_ddpg_softcopy_epsilon_greedy_run_test_log3',\n",
    "        'target_hard_copy_flag': False,\n",
    "        'target_hard_copy_interval': 200,\n",
    "        'tau': 0.001,\n",
    "        'use_gym_monitor': False,\n",
    "        'use_gym_monitor_flag': False}\n",
    "if not os.path.exists(args['summary_dir']):\n",
    "    os.makedirs(args['summary_dir'])\n",
    "log_dir = os.path.join(args['summary_dir'], 'ddpg_running_log.log')\n",
    "logging.basicConfig(filename=log_dir, filemode='a', level=logging.INFO)\n",
    "for key in args.keys():\n",
    "    logging.info('{0}: {1}'.format(key, args[key]))\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
