{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "import logging\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU' or x.device_type=='CPU']\n",
    "tf_availale_devices = get_available_devices()\n",
    "print(tf_availale_devices)\n",
    "\n",
    "# ===========================\n",
    "#   Actor and Critic DNNs\n",
    "# ===========================\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, continuous_act_space_flag, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.continuous_act_space_flag = continuous_act_space_flag\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 300)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        # If Actor acts on discrete action space, use Softmax\n",
    "        if self.continuous_act_space_flag is True:\n",
    "            out = tflearn.fully_connected(\n",
    "                net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        else:\n",
    "            out = tflearn.fully_connected(\n",
    "                net, self.a_dim, activation='softmax', weights_init=w_init)\n",
    "        # Scale output to -action_bound to action_bound\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        # TODO\n",
    "        powerful_critic = True\n",
    "        if powerful_critic is True:\n",
    "            net = tflearn.fully_connected(inputs, 400)\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "            net = tflearn.activations.relu(net)\n",
    "\n",
    "            # Add the action tensor in the 2nd hidden layer\n",
    "            # Use two temp layers to get the corresponding weights and biases\n",
    "            t1 = tflearn.fully_connected(net, 300)\n",
    "            t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "            net = tflearn.activation(\n",
    "                tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "        else:\n",
    "            net = tflearn.fully_connected(inputs, 400)\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "            net = tflearn.activations.relu(net)\n",
    "\n",
    "            # Add the action tensor in the 2nd hidden layer\n",
    "            # Use two temp layers to get the corresponding weights and biases\n",
    "            t1 = tflearn.fully_connected(net, 600)\n",
    "            t2 = tflearn.fully_connected(action, 600)\n",
    "\n",
    "            net = tflearn.activation(\n",
    "                tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "            # # Add the action tensor in the 2nd hidden layer\n",
    "            # # Use two temp layers to get the corresponding weights and biases\n",
    "            # t1 = tflearn.fully_connected(inputs, 300)\n",
    "            # t2 = tflearn.fully_connected(action, 300)\n",
    "            #\n",
    "            # net = tflearn.activation(\n",
    "            #     tf.matmul(inputs, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "        \n",
    "\n",
    "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "# ===========================\n",
    "#   Tensorflow Summary Ops\n",
    "# ===========================\n",
    "\n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "# ===========================\n",
    "#   Agent Training\n",
    "# ===========================\n",
    "\n",
    "def train(sess, env, args, actor, critic, actor_noise):\n",
    "\n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Epsilon parameter\n",
    "    epsilon = args['epsilon_max']\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n",
    "\n",
    "    # Time step\n",
    "    time_step = 0.\n",
    "\n",
    "    # Needed to enable BatchNorm. \n",
    "    # This hurts the performance on Pendulum but could be useful\n",
    "    # in other environments.\n",
    "    # tflearn.is_training(True)\n",
    "\n",
    "    for i in range(int(args['max_episodes'])):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        ep_steps = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            if args['render_env_flag']:\n",
    "                env.render()\n",
    "\n",
    "            # Added exploration noise\n",
    "            #a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i))\n",
    "            # TODO: different exploration strategy\n",
    "            a = []\n",
    "            action = []\n",
    "            exploration_strategy = args['exploration_strategy']\n",
    "            if exploration_strategy == 'action_noise':\n",
    "                a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n",
    "                # Convert continuous action into discrete action\n",
    "                if args['continuous_act_space_flag'] is True:\n",
    "                    action = a[0]\n",
    "                else:\n",
    "                    action = np.argmax(a[0])\n",
    "            elif exploration_strategy == 'epsilon_greedy':\n",
    "                if np.random.rand() < epsilon:\n",
    "                    if args['continuous_act_space_flag'] is True:\n",
    "                        a = np.reshape(env.action_space.sample(), (1, actor.a_dim))\n",
    "                    else:\n",
    "                        a = np.random.uniform(0, 1, (1, actor.a_dim))\n",
    "                else:\n",
    "                    a = actor.predict(np.reshape(s, (1, actor.s_dim)))\n",
    "                # Convert continuous action into discrete action\n",
    "                if args['continuous_act_space_flag'] is True:\n",
    "                    action = a[0]\n",
    "                else:\n",
    "                    action = np.argmax(a[0])\n",
    "            else:\n",
    "                print('Please choose a proper exploration strategy!')\n",
    "\n",
    "            s2, r, terminal, info = env.step(action)\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "\n",
    "            # Reduce epsilon\n",
    "            time_step += 1.\n",
    "            epsilon = args['epsilon_min'] + (args['epsilon_max'] - args['epsilon_min']) * np.exp(-args['epsilon_decay'] * time_step)\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > int(args['minibatch_size']):\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                    replay_buffer.sample_batch(int(args['minibatch_size']))\n",
    "\n",
    "                if args['double_ddpg_flag']:\n",
    "                    # Calculate targets: Double DDPG\n",
    "                    target_q = critic.predict_target(\n",
    "                            s2_batch, actor.predict(s2_batch))\n",
    "                else:\n",
    "                    # Calculate targets\n",
    "                    target_q = critic.predict_target(\n",
    "                            s2_batch, actor.predict_target(s2_batch))\n",
    "                \n",
    "                \n",
    "\n",
    "                y_i = []\n",
    "                for k in range(int(args['minibatch_size'])):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = critic.train(\n",
    "                    s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                a_outs = actor.predict(s_batch)\n",
    "                grads = critic.action_gradients(s_batch, a_outs)\n",
    "                actor.train(s_batch, grads[0])\n",
    "                \n",
    "                # Update target networks\n",
    "                if args['target_hard_copy_flag']:\n",
    "                    if ep_steps % args['target_hard_copy_interval'] == 0:\n",
    "                        actor.update_target_network()\n",
    "                        critic.update_target_network()\n",
    "                else:\n",
    "                    actor.update_target_network()\n",
    "                    critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "            ep_steps += 1\n",
    "\n",
    "            # if terminal or reach maximum length\n",
    "            if terminal:\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float((ep_steps + 1))\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, i)\n",
    "                writer.flush()\n",
    "\n",
    "                ep_stats = '| Episode: {0} | Steps: {1} | Reward: {2:.4f} | Qmax: {3:.4f}'.format(i,\n",
    "                                                                                             (ep_steps + 1),\n",
    "                                                                                             ep_reward,\n",
    "                                                                                             (ep_ave_max_q / float(ep_steps+1)))\n",
    "                print(ep_stats)\n",
    "                logging.info(ep_stats)\n",
    "                break\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        env = gym.make(args['env'])\n",
    "\n",
    "        np.random.seed(int(args['random_seed']))\n",
    "        tf.set_random_seed(int(args['random_seed']))\n",
    "        env.seed(int(args['random_seed']))\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        # Set action_dim for continuous and discrete action space\n",
    "        if args['continuous_act_space_flag'] is True:\n",
    "            action_dim = env.action_space.shape[0]\n",
    "            action_bound = env.action_space.high\n",
    "            # Ensure action bound is symmetric\n",
    "            assert (env.action_space.high == -env.action_space.low).all()\n",
    "        else:\n",
    "            action_dim = env.action_space.n\n",
    "            # If discrete action, actor uses Softmax and action_bound is always 1\n",
    "            action_bound = 1\n",
    "\n",
    "        # Use hardcopy way to update target NNs.\n",
    "        if args['target_hard_copy_flag'] is True:\n",
    "            args['tau'] = 1.0\n",
    "\n",
    "        actor = ActorNetwork(sess, args['continuous_act_space_flag'],\n",
    "                             state_dim, action_dim, action_bound,\n",
    "                             float(args['actor_lr']), float(args['tau']),\n",
    "                             int(args['minibatch_size']))\n",
    "\n",
    "        critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                               float(args['critic_lr']), float(args['tau']),\n",
    "                               float(args['gamma']),\n",
    "                               actor.get_num_trainable_vars())\n",
    "        \n",
    "        actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "        # Record videos\n",
    "        # Use the gym env Monitor wrapper\n",
    "        if args['use_gym_monitor_flag']:\n",
    "            monitor_dir = os.path.join(args['summary_dir'], 'gym_monitor')\n",
    "            env = wrappers.Monitor(env, monitor_dir,\n",
    "                                   resume=True,\n",
    "                                   video_callable=lambda count: count % args['record_video_every'] == 0)\n",
    "\n",
    "        train(sess, env, args, actor, critic, actor_noise)\n",
    "\n",
    "        if args['use_gym_monitor_flag']:\n",
    "            env.monitor.close()\n",
    "        else:\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='provide arguments for DDPG agent')\n",
    "\n",
    "#     # agent parameters\n",
    "#     parser.add_argument('--actor-lr', type=float, default=0.0001, help='actor network learning rate')\n",
    "#     parser.add_argument('--critic-lr', type=float, default=0.001, help='critic network learning rate')\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99, help='discount factor for critic updates')\n",
    "#     parser.add_argument('--tau', type=float, default=0.001, help='soft target update parameter')\n",
    "#     parser.add_argument('--buffer-size', type=int, default=1000000, help='max size of the replay buffer')\n",
    "#     parser.add_argument('--minibatch-size', type=int, default=64, help='size of minibatch for minibatch-SGD')\n",
    "#     parser.add_argument(\"--continuous-act-space-flag\", action=\"store_true\", help='act on continuous action space')\n",
    "\n",
    "#     parser.add_argument(\"--exploration-strategy\", type=str, choices=[\"action_noise\", \"epsilon_greedy\"],\n",
    "#                         default='epsilon_greedy', help='action_noise or epsilon_greedy')\n",
    "#     parser.add_argument(\"--epsilon-max\", type=float, default=1.0, help='maximum of epsilon')\n",
    "#     parser.add_argument(\"--epsilon-min\", type=float, default=.01, help='minimum of epsilon')\n",
    "#     parser.add_argument(\"--epsilon-decay\", type=float, default=.001, help='epsilon decay')\n",
    "\n",
    "#     # train parameters\n",
    "#     parser.add_argument('--double-ddpg-flag', action=\"store_true\", help='True, if run double-ddpg-flag. Otherwise, False.')\n",
    "#     parser.add_argument('--target-hard-copy-flag', action=\"store_true\", help='Target network update method: hard copy')\n",
    "#     parser.add_argument('--target-hard-copy-interval', type=int, default=200, help='Target network update hard copy interval')\n",
    "\n",
    "#     # run parameters\n",
    "#     # HalfCheetah-v2, Ant-v2, InvertedPendulum-v2, Pendulum-v0\n",
    "#     parser.add_argument('--env', type=str, default='HalfCheetah-v2', help='choose the gym env- tested on {Pendulum-v0}')\n",
    "#     parser.add_argument('--random-seed', type=int, default=1234, help='random seed for repeatability')\n",
    "#     parser.add_argument('--max-episodes', type=int, default=50000, help='max num of episodes to do while training')\n",
    "#     # parser.add_argument(\"--max-episode-len\", type=int, default=1000, help='max length of 1 episode')\n",
    "#     parser.add_argument(\"--render-env-flag\", action=\"store_true\", help='render environment')\n",
    "#     parser.add_argument(\"--use-gym-monitor-flag\", action=\"store_true\", help='record gym results')\n",
    "#     parser.add_argument(\"--record-video-every\", type=int, default=1, help='record video every xx episodes')\n",
    "#     parser.add_argument(\"--monitor-dir\", type=str, default='./results/gym_ddpg', help='directory for storing gym results')\n",
    "#     parser.add_argument(\"--summary-dir\", type=str, default='./results/tf_ddpg/HalfCheetah-v2/ddpg_Tau_0.001_run1', help='directory for storing tensorboard info')\n",
    "\n",
    "\n",
    "#     parser.set_defaults(use_gym_monitor=False)\n",
    "\n",
    "#     # args = vars(parser.parse_args())\n",
    "#     # args = parser.parse_args()\n",
    "#     args = vars(parser.parse_args())\n",
    "\n",
    "#     pp.pprint(args)\n",
    "\n",
    "#     if not os.path.exists(args['summary_dir']):\n",
    "#         os.makedirs(args['summary_dir'])\n",
    "#     log_dir = os.path.join(args['summary_dir'], 'ddpg_running_log.log')\n",
    "#     logging.basicConfig(filename=log_dir, filemode='a', level=logging.INFO)\n",
    "#     for key in args.keys():\n",
    "#         logging.info('{0}: {1}'.format(key, args[key]))\n",
    "\n",
    "#     main(args)\n",
    "\n",
    "#     # python ddpg_discrete_action.py --env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name Qmax Value is illegal; using Qmax_Value instead.\n",
      "WARNING:tensorflow:Issue encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "| Episode: 0 | Steps: 1601 | Reward: -76.2517 | Qmax: 0.2419\n",
      "| Episode: 1 | Steps: 98 | Reward: -103.5657 | Qmax: 0.3169\n",
      "| Episode: 2 | Steps: 71 | Reward: -111.1589 | Qmax: 0.1865\n",
      "| Episode: 3 | Steps: 103 | Reward: -117.5819 | Qmax: 0.1167\n",
      "| Episode: 4 | Steps: 110 | Reward: -123.7533 | Qmax: 0.3458\n",
      "| Episode: 5 | Steps: 40 | Reward: -108.4608 | Qmax: 0.4122\n",
      "| Episode: 6 | Steps: 92 | Reward: -112.8875 | Qmax: 0.3144\n",
      "| Episode: 7 | Steps: 142 | Reward: -114.0091 | Qmax: 0.1245\n",
      "| Episode: 8 | Steps: 46 | Reward: -118.3835 | Qmax: 0.0844\n",
      "| Episode: 9 | Steps: 44 | Reward: -122.8355 | Qmax: 0.1138\n",
      "| Episode: 10 | Steps: 43 | Reward: -122.2448 | Qmax: 0.1906\n",
      "| Episode: 11 | Steps: 44 | Reward: -123.1375 | Qmax: 0.1530\n",
      "| Episode: 12 | Steps: 44 | Reward: -121.8675 | Qmax: 0.1844\n",
      "| Episode: 13 | Steps: 57 | Reward: -115.2666 | Qmax: 0.2870\n",
      "| Episode: 14 | Steps: 43 | Reward: -123.3100 | Qmax: 0.2429\n",
      "| Episode: 15 | Steps: 61 | Reward: -116.7503 | Qmax: 0.2797\n",
      "| Episode: 16 | Steps: 186 | Reward: -120.7514 | Qmax: 0.3333\n",
      "| Episode: 17 | Steps: 132 | Reward: -115.5931 | Qmax: 0.4161\n",
      "| Episode: 18 | Steps: 837 | Reward: -196.7175 | Qmax: 0.3582\n",
      "| Episode: 19 | Steps: 101 | Reward: -109.6482 | Qmax: 0.3755\n",
      "| Episode: 20 | Steps: 108 | Reward: -114.0698 | Qmax: 0.3543\n",
      "| Episode: 21 | Steps: 60 | Reward: -105.0646 | Qmax: 0.4024\n",
      "| Episode: 22 | Steps: 55 | Reward: -107.8491 | Qmax: 0.4243\n",
      "| Episode: 23 | Steps: 162 | Reward: -139.9748 | Qmax: 0.5074\n",
      "| Episode: 24 | Steps: 46 | Reward: -106.7030 | Qmax: 0.4693\n",
      "| Episode: 25 | Steps: 47 | Reward: -108.1870 | Qmax: 0.4569\n",
      "| Episode: 26 | Steps: 47 | Reward: -107.1734 | Qmax: 0.7012\n",
      "| Episode: 27 | Steps: 86 | Reward: -103.2350 | Qmax: 0.7114\n",
      "| Episode: 28 | Steps: 83 | Reward: -105.0390 | Qmax: 0.5875\n",
      "| Episode: 29 | Steps: 47 | Reward: -107.1938 | Qmax: 0.7115\n",
      "| Episode: 30 | Steps: 104 | Reward: -106.2872 | Qmax: 0.5608\n",
      "| Episode: 31 | Steps: 46 | Reward: -107.4139 | Qmax: 0.6345\n",
      "| Episode: 32 | Steps: 59 | Reward: -105.3464 | Qmax: 0.6168\n",
      "| Episode: 33 | Steps: 64 | Reward: -103.6431 | Qmax: 0.5495\n",
      "| Episode: 34 | Steps: 60 | Reward: -105.0453 | Qmax: 0.5809\n",
      "| Episode: 35 | Steps: 60 | Reward: -105.0467 | Qmax: 0.5471\n",
      "| Episode: 36 | Steps: 46 | Reward: -108.0262 | Qmax: 0.6984\n",
      "| Episode: 37 | Steps: 67 | Reward: -103.5593 | Qmax: 0.5071\n",
      "| Episode: 38 | Steps: 52 | Reward: -105.1089 | Qmax: 0.6593\n",
      "| Episode: 39 | Steps: 112 | Reward: -135.7916 | Qmax: 0.6482\n",
      "| Episode: 40 | Steps: 65 | Reward: -107.6489 | Qmax: 0.6289\n",
      "| Episode: 41 | Steps: 54 | Reward: -102.0250 | Qmax: 0.6540\n",
      "| Episode: 42 | Steps: 54 | Reward: -102.8361 | Qmax: 0.5447\n",
      "| Episode: 43 | Steps: 105 | Reward: -104.4592 | Qmax: 0.6627\n",
      "| Episode: 44 | Steps: 106 | Reward: -104.6689 | Qmax: 0.7335\n",
      "| Episode: 45 | Steps: 220 | Reward: -116.0541 | Qmax: 0.8429\n",
      "| Episode: 46 | Steps: 114 | Reward: -137.8584 | Qmax: 0.9228\n",
      "| Episode: 47 | Steps: 76 | Reward: -102.4823 | Qmax: 1.0262\n",
      "| Episode: 48 | Steps: 73 | Reward: -102.9106 | Qmax: 0.9233\n",
      "| Episode: 49 | Steps: 47 | Reward: -107.8811 | Qmax: 1.1799\n",
      "| Episode: 50 | Steps: 75 | Reward: -103.1126 | Qmax: 1.1066\n",
      "| Episode: 51 | Steps: 115 | Reward: -110.1609 | Qmax: 1.1897\n",
      "| Episode: 52 | Steps: 65 | Reward: -103.6137 | Qmax: 1.3106\n",
      "| Episode: 53 | Steps: 83 | Reward: -103.8782 | Qmax: 1.0939\n",
      "| Episode: 54 | Steps: 99 | Reward: -104.9104 | Qmax: 1.1816\n",
      "| Episode: 55 | Steps: 80 | Reward: -103.7971 | Qmax: 1.4297\n",
      "| Episode: 56 | Steps: 113 | Reward: -134.3799 | Qmax: 1.2780\n",
      "| Episode: 57 | Steps: 1601 | Reward: -181.4890 | Qmax: 1.6906\n",
      "| Episode: 58 | Steps: 587 | Reward: -169.2206 | Qmax: 2.2826\n",
      "| Episode: 59 | Steps: 127 | Reward: -127.4804 | Qmax: 2.3687\n",
      "| Episode: 60 | Steps: 201 | Reward: -112.9412 | Qmax: 2.4605\n",
      "| Episode: 61 | Steps: 103 | Reward: -125.2565 | Qmax: 2.4791\n",
      "| Episode: 62 | Steps: 170 | Reward: -132.2687 | Qmax: 2.5389\n",
      "| Episode: 63 | Steps: 203 | Reward: -116.0288 | Qmax: 2.5636\n",
      "| Episode: 64 | Steps: 99 | Reward: -124.8734 | Qmax: 2.5256\n",
      "| Episode: 65 | Steps: 175 | Reward: -114.4307 | Qmax: 2.6066\n",
      "| Episode: 66 | Steps: 146 | Reward: -115.3776 | Qmax: 2.4682\n",
      "| Episode: 67 | Steps: 50 | Reward: -116.2141 | Qmax: 2.4393\n",
      "| Episode: 68 | Steps: 80 | Reward: -113.7270 | Qmax: 2.4940\n",
      "| Episode: 69 | Steps: 40 | Reward: -116.9037 | Qmax: 2.5041\n",
      "| Episode: 70 | Steps: 132 | Reward: -114.1265 | Qmax: 2.4108\n",
      "| Episode: 71 | Steps: 40 | Reward: -116.8237 | Qmax: 2.3923\n",
      "| Episode: 72 | Steps: 214 | Reward: -123.2976 | Qmax: 2.4153\n",
      "| Episode: 73 | Steps: 91 | Reward: -124.5143 | Qmax: 2.3665\n",
      "| Episode: 74 | Steps: 74 | Reward: -120.9221 | Qmax: 2.4731\n",
      "| Episode: 75 | Steps: 73 | Reward: -119.0443 | Qmax: 2.3655\n",
      "| Episode: 76 | Steps: 88 | Reward: -127.3060 | Qmax: 2.4600\n",
      "| Episode: 77 | Steps: 139 | Reward: -130.3131 | Qmax: 2.4064\n",
      "| Episode: 78 | Steps: 80 | Reward: -126.3746 | Qmax: 2.2587\n",
      "| Episode: 79 | Steps: 80 | Reward: -124.8685 | Qmax: 2.3076\n",
      "| Episode: 80 | Steps: 60 | Reward: -117.1778 | Qmax: 2.2300\n",
      "| Episode: 81 | Steps: 76 | Reward: -124.4472 | Qmax: 2.3088\n",
      "| Episode: 82 | Steps: 55 | Reward: -116.9003 | Qmax: 2.2483\n",
      "| Episode: 83 | Steps: 72 | Reward: -121.2557 | Qmax: 2.2429\n",
      "| Episode: 84 | Steps: 72 | Reward: -122.5145 | Qmax: 2.2024\n",
      "| Episode: 85 | Steps: 60 | Reward: -117.0382 | Qmax: 2.2971\n",
      "| Episode: 86 | Steps: 80 | Reward: -126.4077 | Qmax: 2.2114\n",
      "| Episode: 87 | Steps: 78 | Reward: -123.2453 | Qmax: 2.2189\n",
      "| Episode: 88 | Steps: 88 | Reward: -128.8255 | Qmax: 2.1502\n",
      "| Episode: 89 | Steps: 57 | Reward: -116.8806 | Qmax: 2.1512\n",
      "| Episode: 90 | Steps: 56 | Reward: -115.7046 | Qmax: 2.1634\n",
      "| Episode: 91 | Steps: 63 | Reward: -117.6471 | Qmax: 2.2246\n",
      "| Episode: 92 | Steps: 70 | Reward: -118.9398 | Qmax: 2.1904\n",
      "| Episode: 93 | Steps: 55 | Reward: -116.0648 | Qmax: 2.1926\n",
      "| Episode: 94 | Steps: 114 | Reward: -134.2306 | Qmax: 2.1869\n",
      "| Episode: 95 | Steps: 99 | Reward: -130.9689 | Qmax: 2.1686\n",
      "| Episode: 96 | Steps: 62 | Reward: -115.9388 | Qmax: 2.1355\n",
      "| Episode: 97 | Steps: 92 | Reward: -128.6396 | Qmax: 2.1502\n",
      "| Episode: 98 | Steps: 92 | Reward: -127.9110 | Qmax: 2.0886\n",
      "| Episode: 99 | Steps: 106 | Reward: -131.2747 | Qmax: 2.1580\n",
      "| Episode: 100 | Steps: 159 | Reward: -139.7372 | Qmax: 2.1738\n",
      "| Episode: 101 | Steps: 73 | Reward: -122.7658 | Qmax: 2.1385\n",
      "| Episode: 102 | Steps: 58 | Reward: -119.7652 | Qmax: 2.1241\n",
      "| Episode: 103 | Steps: 107 | Reward: -131.0399 | Qmax: 2.2009\n",
      "| Episode: 104 | Steps: 59 | Reward: -120.8766 | Qmax: 2.1193\n",
      "| Episode: 105 | Steps: 71 | Reward: -121.9410 | Qmax: 2.1030\n",
      "| Episode: 106 | Steps: 143 | Reward: -119.7040 | Qmax: 2.1191\n",
      "| Episode: 107 | Steps: 75 | Reward: -120.0558 | Qmax: 2.1537\n",
      "| Episode: 108 | Steps: 99 | Reward: -129.4171 | Qmax: 2.1352\n",
      "| Episode: 109 | Steps: 92 | Reward: -126.2836 | Qmax: 2.1525\n",
      "| Episode: 110 | Steps: 141 | Reward: -117.5384 | Qmax: 2.1416\n",
      "| Episode: 111 | Steps: 78 | Reward: -115.6990 | Qmax: 2.0833\n",
      "| Episode: 112 | Steps: 81 | Reward: -123.4851 | Qmax: 2.0951\n",
      "| Episode: 113 | Steps: 71 | Reward: -116.8428 | Qmax: 2.1481\n",
      "| Episode: 114 | Steps: 71 | Reward: -121.1350 | Qmax: 2.0909\n",
      "| Episode: 115 | Steps: 66 | Reward: -120.0070 | Qmax: 2.0794\n",
      "| Episode: 116 | Steps: 60 | Reward: -116.5564 | Qmax: 2.0979\n",
      "| Episode: 117 | Steps: 68 | Reward: -118.9121 | Qmax: 2.1209\n",
      "| Episode: 118 | Steps: 73 | Reward: -114.1424 | Qmax: 2.0913\n",
      "| Episode: 119 | Steps: 106 | Reward: -129.1278 | Qmax: 2.1431\n",
      "| Episode: 120 | Steps: 46 | Reward: -111.5932 | Qmax: 2.0520\n",
      "| Episode: 121 | Steps: 61 | Reward: -115.4307 | Qmax: 2.0808\n",
      "| Episode: 122 | Steps: 121 | Reward: -130.2166 | Qmax: 2.0626\n",
      "| Episode: 123 | Steps: 82 | Reward: -117.1695 | Qmax: 2.0736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 124 | Steps: 63 | Reward: -117.6260 | Qmax: 2.0487\n",
      "| Episode: 125 | Steps: 76 | Reward: -122.9466 | Qmax: 2.0138\n",
      "| Episode: 126 | Steps: 87 | Reward: -125.5761 | Qmax: 2.0352\n",
      "| Episode: 127 | Steps: 55 | Reward: -115.7613 | Qmax: 1.9968\n",
      "| Episode: 128 | Steps: 69 | Reward: -114.0456 | Qmax: 1.9998\n",
      "| Episode: 129 | Steps: 152 | Reward: -125.4020 | Qmax: 2.0607\n",
      "| Episode: 130 | Steps: 36 | Reward: -108.1323 | Qmax: 1.9043\n",
      "| Episode: 131 | Steps: 69 | Reward: -120.7730 | Qmax: 1.9793\n",
      "| Episode: 132 | Steps: 85 | Reward: -124.7239 | Qmax: 1.9756\n",
      "| Episode: 133 | Steps: 63 | Reward: -119.1100 | Qmax: 1.9343\n",
      "| Episode: 134 | Steps: 51 | Reward: -113.3851 | Qmax: 1.9525\n",
      "| Episode: 135 | Steps: 58 | Reward: -115.5652 | Qmax: 1.8616\n",
      "| Episode: 136 | Steps: 74 | Reward: -122.1303 | Qmax: 2.0193\n",
      "| Episode: 137 | Steps: 68 | Reward: -118.7411 | Qmax: 1.8969\n",
      "| Episode: 138 | Steps: 91 | Reward: -126.0656 | Qmax: 1.8987\n",
      "| Episode: 139 | Steps: 75 | Reward: -111.4063 | Qmax: 1.8225\n",
      "| Episode: 140 | Steps: 87 | Reward: -111.1137 | Qmax: 1.8475\n",
      "| Episode: 141 | Steps: 94 | Reward: -110.9646 | Qmax: 1.8504\n",
      "| Episode: 142 | Steps: 83 | Reward: -112.8378 | Qmax: 1.8634\n",
      "| Episode: 143 | Steps: 83 | Reward: -110.4523 | Qmax: 1.8331\n",
      "| Episode: 144 | Steps: 80 | Reward: -111.0365 | Qmax: 1.8442\n",
      "| Episode: 145 | Steps: 81 | Reward: -110.6108 | Qmax: 1.8306\n",
      "| Episode: 146 | Steps: 84 | Reward: -110.5902 | Qmax: 1.8609\n",
      "| Episode: 147 | Steps: 71 | Reward: -112.4856 | Qmax: 1.8380\n",
      "| Episode: 148 | Steps: 82 | Reward: -112.5808 | Qmax: 1.8493\n",
      "| Episode: 149 | Steps: 66 | Reward: -112.6540 | Qmax: 1.7348\n",
      "| Episode: 150 | Steps: 76 | Reward: -112.0786 | Qmax: 1.8434\n",
      "| Episode: 151 | Steps: 90 | Reward: -114.3211 | Qmax: 1.8328\n",
      "| Episode: 152 | Steps: 77 | Reward: -110.3119 | Qmax: 1.8496\n",
      "| Episode: 153 | Steps: 83 | Reward: -110.0321 | Qmax: 1.7459\n",
      "| Episode: 154 | Steps: 81 | Reward: -110.5455 | Qmax: 1.7060\n",
      "| Episode: 155 | Steps: 77 | Reward: -110.6626 | Qmax: 1.7576\n",
      "| Episode: 156 | Steps: 79 | Reward: -111.3768 | Qmax: 1.7242\n",
      "| Episode: 157 | Steps: 74 | Reward: -110.6502 | Qmax: 1.7357\n",
      "| Episode: 158 | Steps: 77 | Reward: -111.0385 | Qmax: 1.6854\n",
      "| Episode: 159 | Steps: 77 | Reward: -110.3697 | Qmax: 1.7041\n",
      "| Episode: 160 | Steps: 77 | Reward: -110.3217 | Qmax: 1.6888\n",
      "| Episode: 161 | Steps: 68 | Reward: -111.6450 | Qmax: 1.6894\n",
      "| Episode: 162 | Steps: 67 | Reward: -110.7365 | Qmax: 1.7567\n",
      "| Episode: 163 | Steps: 77 | Reward: -113.1624 | Qmax: 1.7331\n",
      "| Episode: 164 | Steps: 70 | Reward: -111.3321 | Qmax: 1.7254\n",
      "| Episode: 165 | Steps: 68 | Reward: -111.2245 | Qmax: 1.7471\n",
      "| Episode: 166 | Steps: 76 | Reward: -111.3920 | Qmax: 1.6833\n",
      "| Episode: 167 | Steps: 74 | Reward: -110.8498 | Qmax: 1.6125\n",
      "| Episode: 168 | Steps: 73 | Reward: -110.7482 | Qmax: 1.7080\n",
      "| Episode: 169 | Steps: 72 | Reward: -111.1185 | Qmax: 1.6411\n",
      "| Episode: 170 | Steps: 75 | Reward: -110.5476 | Qmax: 1.6860\n",
      "| Episode: 171 | Steps: 69 | Reward: -110.7773 | Qmax: 1.7237\n",
      "| Episode: 172 | Steps: 69 | Reward: -110.6869 | Qmax: 1.7022\n",
      "| Episode: 173 | Steps: 73 | Reward: -110.8181 | Qmax: 1.6734\n",
      "| Episode: 174 | Steps: 71 | Reward: -110.6480 | Qmax: 1.6991\n",
      "| Episode: 175 | Steps: 74 | Reward: -110.1463 | Qmax: 1.6368\n",
      "| Episode: 176 | Steps: 75 | Reward: -110.7100 | Qmax: 1.6358\n",
      "| Episode: 177 | Steps: 74 | Reward: -110.9609 | Qmax: 1.6767\n",
      "| Episode: 178 | Steps: 81 | Reward: -115.1704 | Qmax: 1.6244\n",
      "| Episode: 179 | Steps: 97 | Reward: -126.9669 | Qmax: 1.6931\n",
      "| Episode: 180 | Steps: 45 | Reward: -126.0975 | Qmax: 1.6781\n",
      "| Episode: 181 | Steps: 45 | Reward: -127.8433 | Qmax: 1.6607\n",
      "| Episode: 182 | Steps: 44 | Reward: -127.2861 | Qmax: 1.6209\n",
      "| Episode: 183 | Steps: 78 | Reward: -111.6739 | Qmax: 1.6818\n",
      "| Episode: 184 | Steps: 188 | Reward: -139.7584 | Qmax: 1.7060\n",
      "| Episode: 185 | Steps: 100 | Reward: -129.6599 | Qmax: 1.6753\n",
      "| Episode: 186 | Steps: 44 | Reward: -126.4464 | Qmax: 1.7365\n",
      "| Episode: 187 | Steps: 43 | Reward: -125.7267 | Qmax: 1.7142\n",
      "| Episode: 188 | Steps: 137 | Reward: -116.3295 | Qmax: 1.7210\n",
      "| Episode: 189 | Steps: 44 | Reward: -126.1778 | Qmax: 1.6842\n",
      "| Episode: 190 | Steps: 44 | Reward: -126.0789 | Qmax: 1.7781\n",
      "| Episode: 191 | Steps: 44 | Reward: -126.2800 | Qmax: 1.7261\n",
      "| Episode: 192 | Steps: 45 | Reward: -126.9011 | Qmax: 1.7063\n",
      "| Episode: 193 | Steps: 44 | Reward: -126.4224 | Qmax: 1.7596\n",
      "| Episode: 194 | Steps: 44 | Reward: -126.2632 | Qmax: 1.8702\n",
      "| Episode: 195 | Steps: 45 | Reward: -126.4408 | Qmax: 1.8840\n",
      "| Episode: 196 | Steps: 44 | Reward: -126.2706 | Qmax: 1.7146\n",
      "| Episode: 197 | Steps: 45 | Reward: -127.3615 | Qmax: 1.7741\n",
      "| Episode: 198 | Steps: 149 | Reward: -136.6418 | Qmax: 1.7641\n",
      "| Episode: 199 | Steps: 45 | Reward: -126.6996 | Qmax: 1.7732\n",
      "| Episode: 200 | Steps: 45 | Reward: -128.0506 | Qmax: 1.8678\n",
      "| Episode: 201 | Steps: 45 | Reward: -126.7356 | Qmax: 1.7406\n",
      "| Episode: 202 | Steps: 45 | Reward: -126.5652 | Qmax: 1.8355\n",
      "| Episode: 203 | Steps: 45 | Reward: -126.5920 | Qmax: 1.7738\n",
      "| Episode: 204 | Steps: 45 | Reward: -126.4780 | Qmax: 1.7486\n",
      "| Episode: 205 | Steps: 45 | Reward: -126.7909 | Qmax: 1.7620\n",
      "| Episode: 206 | Steps: 45 | Reward: -126.5681 | Qmax: 1.7667\n",
      "| Episode: 207 | Steps: 45 | Reward: -126.3360 | Qmax: 1.7594\n",
      "| Episode: 208 | Steps: 45 | Reward: -126.8220 | Qmax: 1.8517\n",
      "| Episode: 209 | Steps: 44 | Reward: -126.5229 | Qmax: 1.8139\n",
      "| Episode: 210 | Steps: 112 | Reward: -114.0880 | Qmax: 1.8897\n",
      "| Episode: 211 | Steps: 44 | Reward: -126.1962 | Qmax: 1.8809\n",
      "| Episode: 212 | Steps: 45 | Reward: -128.0859 | Qmax: 1.8088\n",
      "| Episode: 213 | Steps: 45 | Reward: -127.8553 | Qmax: 1.9355\n",
      "| Episode: 214 | Steps: 45 | Reward: -126.8345 | Qmax: 1.9508\n",
      "| Episode: 215 | Steps: 45 | Reward: -127.9195 | Qmax: 1.9291\n",
      "| Episode: 216 | Steps: 45 | Reward: -127.7540 | Qmax: 1.9374\n",
      "| Episode: 217 | Steps: 45 | Reward: -126.6644 | Qmax: 1.9483\n",
      "| Episode: 218 | Steps: 45 | Reward: -128.1970 | Qmax: 1.9365\n",
      "| Episode: 219 | Steps: 45 | Reward: -128.0015 | Qmax: 2.0100\n",
      "| Episode: 220 | Steps: 45 | Reward: -127.8315 | Qmax: 1.9102\n",
      "| Episode: 221 | Steps: 46 | Reward: -127.8440 | Qmax: 1.8903\n",
      "| Episode: 222 | Steps: 44 | Reward: -126.1542 | Qmax: 1.9315\n",
      "| Episode: 223 | Steps: 84 | Reward: -112.5912 | Qmax: 2.0043\n",
      "| Episode: 224 | Steps: 44 | Reward: -126.3486 | Qmax: 1.9527\n",
      "| Episode: 225 | Steps: 44 | Reward: -126.4677 | Qmax: 1.9058\n",
      "| Episode: 226 | Steps: 44 | Reward: -126.7723 | Qmax: 1.9847\n",
      "| Episode: 227 | Steps: 44 | Reward: -126.5137 | Qmax: 2.1670\n",
      "| Episode: 228 | Steps: 44 | Reward: -126.0692 | Qmax: 1.9150\n",
      "| Episode: 229 | Steps: 44 | Reward: -126.3449 | Qmax: 1.8546\n",
      "| Episode: 230 | Steps: 44 | Reward: -126.3637 | Qmax: 2.1291\n",
      "| Episode: 231 | Steps: 44 | Reward: -125.6054 | Qmax: 2.1024\n",
      "| Episode: 232 | Steps: 44 | Reward: -125.6091 | Qmax: 1.8828\n",
      "| Episode: 233 | Steps: 44 | Reward: -124.8992 | Qmax: 1.9872\n",
      "| Episode: 234 | Steps: 161 | Reward: -140.4801 | Qmax: 2.0030\n",
      "| Episode: 235 | Steps: 45 | Reward: -126.9080 | Qmax: 2.0140\n",
      "| Episode: 236 | Steps: 432 | Reward: -160.6868 | Qmax: 2.0043\n",
      "| Episode: 237 | Steps: 226 | Reward: -141.4509 | Qmax: 2.0572\n",
      "| Episode: 238 | Steps: 1601 | Reward: -163.3980 | Qmax: 2.1493\n",
      "| Episode: 239 | Steps: 1601 | Reward: -167.5829 | Qmax: 2.3975\n",
      "| Episode: 240 | Steps: 1601 | Reward: -180.6839 | Qmax: 2.6410\n",
      "| Episode: 241 | Steps: 76 | Reward: -107.0117 | Qmax: 2.8041\n",
      "| Episode: 242 | Steps: 1538 | Reward: -281.4349 | Qmax: 3.1441\n",
      "| Episode: 243 | Steps: 64 | Reward: -109.6485 | Qmax: 3.2670\n",
      "| Episode: 244 | Steps: 68 | Reward: -108.1076 | Qmax: 3.1768\n",
      "| Episode: 245 | Steps: 70 | Reward: -116.9341 | Qmax: 3.0588\n",
      "| Episode: 246 | Steps: 82 | Reward: -120.5342 | Qmax: 3.1602\n",
      "| Episode: 247 | Steps: 87 | Reward: -119.3800 | Qmax: 3.2498\n",
      "| Episode: 248 | Steps: 130 | Reward: -118.1473 | Qmax: 3.2421\n",
      "| Episode: 249 | Steps: 133 | Reward: -119.4876 | Qmax: 3.2887\n",
      "| Episode: 250 | Steps: 131 | Reward: -118.3623 | Qmax: 3.2612\n",
      "| Episode: 251 | Steps: 127 | Reward: -115.4477 | Qmax: 3.2779\n",
      "| Episode: 252 | Steps: 125 | Reward: -121.9733 | Qmax: 3.2410\n",
      "| Episode: 253 | Steps: 125 | Reward: -117.5089 | Qmax: 3.2407\n",
      "| Episode: 254 | Steps: 134 | Reward: -119.6027 | Qmax: 3.3415\n",
      "| Episode: 255 | Steps: 116 | Reward: -121.3960 | Qmax: 3.2866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 256 | Steps: 136 | Reward: -118.6800 | Qmax: 3.3403\n",
      "| Episode: 257 | Steps: 121 | Reward: -110.3148 | Qmax: 3.3006\n",
      "| Episode: 258 | Steps: 115 | Reward: -110.3377 | Qmax: 3.3425\n",
      "| Episode: 259 | Steps: 130 | Reward: -120.8491 | Qmax: 3.2996\n",
      "| Episode: 260 | Steps: 118 | Reward: -111.4995 | Qmax: 3.2465\n",
      "| Episode: 261 | Steps: 128 | Reward: -121.4823 | Qmax: 3.3321\n",
      "| Episode: 262 | Steps: 129 | Reward: -120.5233 | Qmax: 3.2719\n",
      "| Episode: 263 | Steps: 131 | Reward: -119.9772 | Qmax: 3.2395\n",
      "| Episode: 264 | Steps: 129 | Reward: -118.6088 | Qmax: 3.3046\n",
      "| Episode: 265 | Steps: 125 | Reward: -118.9484 | Qmax: 3.3827\n",
      "| Episode: 266 | Steps: 128 | Reward: -121.3448 | Qmax: 3.3073\n",
      "| Episode: 267 | Steps: 116 | Reward: -119.0897 | Qmax: 3.4044\n",
      "| Episode: 268 | Steps: 130 | Reward: -118.7560 | Qmax: 3.3944\n",
      "| Episode: 269 | Steps: 131 | Reward: -119.0819 | Qmax: 3.3397\n",
      "| Episode: 270 | Steps: 126 | Reward: -116.6901 | Qmax: 3.3233\n",
      "| Episode: 271 | Steps: 111 | Reward: -113.3349 | Qmax: 3.3784\n",
      "| Episode: 272 | Steps: 121 | Reward: -113.7566 | Qmax: 3.2676\n",
      "| Episode: 273 | Steps: 129 | Reward: -113.5676 | Qmax: 3.3353\n",
      "| Episode: 274 | Steps: 130 | Reward: -114.2073 | Qmax: 3.3479\n",
      "| Episode: 275 | Steps: 130 | Reward: -113.9584 | Qmax: 3.2781\n",
      "| Episode: 276 | Steps: 126 | Reward: -113.1246 | Qmax: 3.2705\n",
      "| Episode: 277 | Steps: 133 | Reward: -113.8306 | Qmax: 3.3899\n",
      "| Episode: 278 | Steps: 125 | Reward: -113.5261 | Qmax: 3.4467\n",
      "| Episode: 279 | Steps: 128 | Reward: -113.6113 | Qmax: 3.3154\n",
      "| Episode: 280 | Steps: 127 | Reward: -113.2117 | Qmax: 3.3182\n",
      "| Episode: 281 | Steps: 69 | Reward: -114.9483 | Qmax: 3.3234\n",
      "| Episode: 282 | Steps: 67 | Reward: -115.2506 | Qmax: 3.2679\n",
      "| Episode: 283 | Steps: 79 | Reward: -116.6838 | Qmax: 3.4544\n",
      "| Episode: 284 | Steps: 85 | Reward: -115.1518 | Qmax: 3.3514\n",
      "| Episode: 285 | Steps: 85 | Reward: -115.1986 | Qmax: 3.4302\n",
      "| Episode: 286 | Steps: 83 | Reward: -117.7758 | Qmax: 3.4153\n",
      "| Episode: 287 | Steps: 83 | Reward: -115.7505 | Qmax: 3.3448\n",
      "| Episode: 288 | Steps: 56 | Reward: -111.4462 | Qmax: 3.4055\n",
      "| Episode: 289 | Steps: 100 | Reward: -116.7764 | Qmax: 3.3893\n",
      "| Episode: 290 | Steps: 79 | Reward: -119.3979 | Qmax: 3.3764\n",
      "| Episode: 291 | Steps: 132 | Reward: -111.4861 | Qmax: 3.3749\n",
      "| Episode: 292 | Steps: 72 | Reward: -117.9703 | Qmax: 3.4076\n",
      "| Episode: 293 | Steps: 77 | Reward: -116.7138 | Qmax: 3.2502\n",
      "| Episode: 294 | Steps: 74 | Reward: -117.6792 | Qmax: 3.3851\n",
      "| Episode: 295 | Steps: 113 | Reward: -109.4275 | Qmax: 3.4376\n",
      "| Episode: 296 | Steps: 128 | Reward: -113.9564 | Qmax: 3.4233\n",
      "| Episode: 297 | Steps: 122 | Reward: -111.9006 | Qmax: 3.3567\n",
      "| Episode: 298 | Steps: 124 | Reward: -114.3002 | Qmax: 3.4512\n",
      "| Episode: 299 | Steps: 130 | Reward: -113.7246 | Qmax: 3.4040\n",
      "| Episode: 300 | Steps: 136 | Reward: -112.5061 | Qmax: 3.3879\n",
      "| Episode: 301 | Steps: 146 | Reward: -113.1976 | Qmax: 3.4517\n",
      "| Episode: 302 | Steps: 129 | Reward: -114.7504 | Qmax: 3.3371\n",
      "| Episode: 303 | Steps: 126 | Reward: -114.1405 | Qmax: 3.3798\n",
      "| Episode: 304 | Steps: 117 | Reward: -112.3912 | Qmax: 3.4187\n",
      "| Episode: 305 | Steps: 124 | Reward: -115.7163 | Qmax: 3.3774\n",
      "| Episode: 306 | Steps: 115 | Reward: -110.1596 | Qmax: 3.4291\n",
      "| Episode: 307 | Steps: 83 | Reward: -119.0252 | Qmax: 3.5925\n",
      "| Episode: 308 | Steps: 112 | Reward: -110.0512 | Qmax: 3.5078\n",
      "| Episode: 309 | Steps: 125 | Reward: -116.3873 | Qmax: 3.5534\n",
      "| Episode: 310 | Steps: 139 | Reward: -116.3801 | Qmax: 3.5174\n",
      "| Episode: 311 | Steps: 123 | Reward: -118.6881 | Qmax: 3.4590\n",
      "| Episode: 312 | Steps: 123 | Reward: -115.5281 | Qmax: 3.6166\n",
      "| Episode: 313 | Steps: 123 | Reward: -117.6614 | Qmax: 3.5581\n",
      "| Episode: 314 | Steps: 120 | Reward: -118.3974 | Qmax: 3.6738\n",
      "| Episode: 315 | Steps: 51 | Reward: -111.1609 | Qmax: 3.5566\n",
      "| Episode: 316 | Steps: 126 | Reward: -119.1753 | Qmax: 3.6695\n",
      "| Episode: 317 | Steps: 122 | Reward: -116.1891 | Qmax: 3.6341\n",
      "| Episode: 318 | Steps: 119 | Reward: -119.1236 | Qmax: 3.6717\n",
      "| Episode: 319 | Steps: 118 | Reward: -114.2410 | Qmax: 3.6822\n",
      "| Episode: 320 | Steps: 124 | Reward: -114.2331 | Qmax: 3.7231\n",
      "| Episode: 321 | Steps: 69 | Reward: -113.4376 | Qmax: 3.5592\n",
      "| Episode: 322 | Steps: 71 | Reward: -112.7328 | Qmax: 3.6426\n",
      "| Episode: 323 | Steps: 73 | Reward: -113.2584 | Qmax: 3.7094\n",
      "| Episode: 324 | Steps: 69 | Reward: -113.1154 | Qmax: 3.6472\n",
      "| Episode: 325 | Steps: 73 | Reward: -113.9043 | Qmax: 3.6247\n",
      "| Episode: 326 | Steps: 76 | Reward: -115.5472 | Qmax: 3.7514\n",
      "| Episode: 327 | Steps: 76 | Reward: -115.6288 | Qmax: 3.7683\n",
      "| Episode: 328 | Steps: 52 | Reward: -110.4390 | Qmax: 3.7913\n",
      "| Episode: 329 | Steps: 52 | Reward: -109.6954 | Qmax: 3.6793\n",
      "| Episode: 330 | Steps: 119 | Reward: -111.3435 | Qmax: 3.8105\n",
      "| Episode: 331 | Steps: 76 | Reward: -113.4806 | Qmax: 3.7868\n",
      "| Episode: 332 | Steps: 93 | Reward: -115.7374 | Qmax: 3.7650\n",
      "| Episode: 333 | Steps: 63 | Reward: -107.5893 | Qmax: 3.7478\n",
      "| Episode: 334 | Steps: 103 | Reward: -110.8023 | Qmax: 3.8985\n",
      "| Episode: 335 | Steps: 70 | Reward: -107.8668 | Qmax: 3.7311\n",
      "| Episode: 336 | Steps: 110 | Reward: -105.9903 | Qmax: 3.8039\n",
      "| Episode: 337 | Steps: 100 | Reward: -107.0440 | Qmax: 3.8522\n",
      "| Episode: 338 | Steps: 74 | Reward: -106.0761 | Qmax: 3.7470\n",
      "| Episode: 339 | Steps: 105 | Reward: -114.3737 | Qmax: 3.8725\n",
      "| Episode: 340 | Steps: 101 | Reward: -112.0198 | Qmax: 3.9357\n",
      "| Episode: 341 | Steps: 91 | Reward: -105.1626 | Qmax: 3.9560\n",
      "| Episode: 342 | Steps: 67 | Reward: -113.3796 | Qmax: 3.8547\n",
      "| Episode: 343 | Steps: 100 | Reward: -112.5424 | Qmax: 3.9435\n",
      "| Episode: 344 | Steps: 69 | Reward: -105.7264 | Qmax: 3.7653\n",
      "| Episode: 345 | Steps: 57 | Reward: -109.0652 | Qmax: 3.9174\n",
      "| Episode: 346 | Steps: 58 | Reward: -110.1186 | Qmax: 3.8869\n",
      "| Episode: 347 | Steps: 60 | Reward: -109.3610 | Qmax: 3.9454\n",
      "| Episode: 348 | Steps: 56 | Reward: -110.0948 | Qmax: 3.8863\n",
      "| Episode: 349 | Steps: 79 | Reward: -103.5215 | Qmax: 4.0842\n",
      "| Episode: 350 | Steps: 77 | Reward: -104.5242 | Qmax: 3.8892\n",
      "| Episode: 351 | Steps: 54 | Reward: -112.1074 | Qmax: 4.1306\n",
      "| Episode: 352 | Steps: 65 | Reward: -109.4950 | Qmax: 4.0525\n",
      "| Episode: 353 | Steps: 59 | Reward: -110.4901 | Qmax: 3.9915\n",
      "| Episode: 354 | Steps: 82 | Reward: -106.1969 | Qmax: 4.0119\n",
      "| Episode: 355 | Steps: 70 | Reward: -104.5877 | Qmax: 3.9832\n",
      "| Episode: 356 | Steps: 63 | Reward: -108.6375 | Qmax: 4.0413\n",
      "| Episode: 357 | Steps: 73 | Reward: -107.6942 | Qmax: 4.0412\n",
      "| Episode: 358 | Steps: 65 | Reward: -107.0296 | Qmax: 3.9949\n",
      "| Episode: 359 | Steps: 82 | Reward: -103.4794 | Qmax: 4.2354\n",
      "| Episode: 360 | Steps: 76 | Reward: -104.5894 | Qmax: 4.0558\n",
      "| Episode: 361 | Steps: 66 | Reward: -106.9245 | Qmax: 4.2045\n",
      "| Episode: 362 | Steps: 75 | Reward: -104.6373 | Qmax: 4.2714\n",
      "| Episode: 363 | Steps: 61 | Reward: -105.0963 | Qmax: 4.1751\n",
      "| Episode: 364 | Steps: 69 | Reward: -104.3669 | Qmax: 4.2766\n",
      "| Episode: 365 | Steps: 64 | Reward: -103.3962 | Qmax: 4.2240\n",
      "| Episode: 366 | Steps: 76 | Reward: -104.3304 | Qmax: 4.3181\n",
      "| Episode: 367 | Steps: 64 | Reward: -101.9787 | Qmax: 4.3349\n",
      "| Episode: 368 | Steps: 75 | Reward: -101.8919 | Qmax: 4.3292\n",
      "| Episode: 369 | Steps: 47 | Reward: -107.1113 | Qmax: 4.2495\n",
      "| Episode: 370 | Steps: 71 | Reward: -106.9286 | Qmax: 4.1918\n",
      "| Episode: 371 | Steps: 101 | Reward: -104.0921 | Qmax: 4.3655\n",
      "| Episode: 372 | Steps: 100 | Reward: -106.3115 | Qmax: 4.4225\n",
      "| Episode: 373 | Steps: 113 | Reward: -105.3322 | Qmax: 4.3376\n",
      "| Episode: 374 | Steps: 103 | Reward: -105.7688 | Qmax: 4.3776\n",
      "| Episode: 375 | Steps: 107 | Reward: -106.5588 | Qmax: 4.3618\n",
      "| Episode: 376 | Steps: 113 | Reward: -104.1734 | Qmax: 4.5083\n",
      "| Episode: 377 | Steps: 97 | Reward: -104.3098 | Qmax: 4.4235\n",
      "| Episode: 378 | Steps: 114 | Reward: -104.8886 | Qmax: 4.3954\n",
      "| Episode: 379 | Steps: 104 | Reward: -105.6543 | Qmax: 4.4544\n",
      "| Episode: 380 | Steps: 105 | Reward: -105.2992 | Qmax: 4.4738\n",
      "| Episode: 381 | Steps: 95 | Reward: -104.3566 | Qmax: 4.5436\n",
      "| Episode: 382 | Steps: 90 | Reward: -106.7807 | Qmax: 4.4829\n",
      "| Episode: 383 | Steps: 95 | Reward: -102.8897 | Qmax: 4.6669\n",
      "| Episode: 384 | Steps: 114 | Reward: -103.5201 | Qmax: 4.5769\n",
      "| Episode: 385 | Steps: 51 | Reward: -107.2321 | Qmax: 4.4590\n",
      "| Episode: 386 | Steps: 111 | Reward: -105.3963 | Qmax: 4.6436\n",
      "| Episode: 387 | Steps: 47 | Reward: -107.9505 | Qmax: 4.5295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 388 | Steps: 48 | Reward: -108.2156 | Qmax: 4.5697\n",
      "| Episode: 389 | Steps: 47 | Reward: -108.2063 | Qmax: 4.5104\n",
      "| Episode: 390 | Steps: 76 | Reward: -101.3248 | Qmax: 4.6095\n",
      "| Episode: 391 | Steps: 290 | Reward: -93.2627 | Qmax: 4.7634\n",
      "| Episode: 392 | Steps: 111 | Reward: -108.2995 | Qmax: 4.7465\n",
      "| Episode: 393 | Steps: 1601 | Reward: -162.6505 | Qmax: 5.0178\n",
      "| Episode: 394 | Steps: 175 | Reward: -101.5538 | Qmax: 5.1592\n",
      "| Episode: 395 | Steps: 1453 | Reward: -238.7900 | Qmax: 5.4807\n",
      "| Episode: 396 | Steps: 1601 | Reward: -182.8002 | Qmax: 5.6684\n",
      "| Episode: 397 | Steps: 68 | Reward: -102.0076 | Qmax: 5.6866\n",
      "| Episode: 398 | Steps: 67 | Reward: -102.0628 | Qmax: 5.6459\n",
      "| Episode: 399 | Steps: 1601 | Reward: -180.5753 | Qmax: 5.9467\n",
      "| Episode: 400 | Steps: 69 | Reward: -101.4613 | Qmax: 5.9732\n",
      "| Episode: 401 | Steps: 73 | Reward: -102.4219 | Qmax: 5.8097\n",
      "| Episode: 402 | Steps: 68 | Reward: -101.7660 | Qmax: 5.8343\n",
      "| Episode: 403 | Steps: 69 | Reward: -101.8344 | Qmax: 6.0291\n",
      "| Episode: 404 | Steps: 1601 | Reward: -179.7863 | Qmax: 6.1085\n",
      "| Episode: 405 | Steps: 65 | Reward: -105.7577 | Qmax: 5.9888\n",
      "| Episode: 406 | Steps: 1601 | Reward: -181.2410 | Qmax: 6.2539\n",
      "| Episode: 407 | Steps: 1601 | Reward: -179.8571 | Qmax: 6.6851\n",
      "| Episode: 408 | Steps: 1601 | Reward: -182.5952 | Qmax: 7.0355\n",
      "| Episode: 409 | Steps: 81 | Reward: -102.8492 | Qmax: 7.1409\n",
      "| Episode: 410 | Steps: 119 | Reward: -103.5676 | Qmax: 7.2692\n",
      "| Episode: 411 | Steps: 87 | Reward: -102.4026 | Qmax: 6.9169\n",
      "| Episode: 412 | Steps: 146 | Reward: -133.6408 | Qmax: 7.0463\n",
      "| Episode: 413 | Steps: 74 | Reward: -103.5030 | Qmax: 7.2608\n",
      "| Episode: 414 | Steps: 89 | Reward: -102.3530 | Qmax: 7.1012\n",
      "| Episode: 415 | Steps: 1357 | Reward: -242.1169 | Qmax: 7.5878\n",
      "| Episode: 416 | Steps: 55 | Reward: -104.7644 | Qmax: 7.4085\n",
      "| Episode: 417 | Steps: 67 | Reward: -105.0567 | Qmax: 7.6604\n",
      "| Episode: 418 | Steps: 73 | Reward: -104.6908 | Qmax: 7.5342\n",
      "| Episode: 419 | Steps: 73 | Reward: -106.3610 | Qmax: 7.9068\n",
      "| Episode: 420 | Steps: 76 | Reward: -104.9731 | Qmax: 7.8388\n",
      "| Episode: 421 | Steps: 67 | Reward: -101.1511 | Qmax: 7.7814\n",
      "| Episode: 422 | Steps: 70 | Reward: -101.0125 | Qmax: 7.5737\n",
      "| Episode: 423 | Steps: 60 | Reward: -104.3881 | Qmax: 7.9162\n",
      "| Episode: 424 | Steps: 52 | Reward: -104.2133 | Qmax: 7.4785\n",
      "| Episode: 425 | Steps: 57 | Reward: -103.6576 | Qmax: 7.7149\n",
      "| Episode: 426 | Steps: 65 | Reward: -104.8955 | Qmax: 7.6499\n",
      "| Episode: 427 | Steps: 61 | Reward: -104.3349 | Qmax: 8.0059\n",
      "| Episode: 428 | Steps: 64 | Reward: -100.7924 | Qmax: 7.8156\n",
      "| Episode: 429 | Steps: 56 | Reward: -103.4916 | Qmax: 7.7743\n",
      "| Episode: 430 | Steps: 78 | Reward: -100.3254 | Qmax: 7.6466\n",
      "| Episode: 431 | Steps: 64 | Reward: -102.4893 | Qmax: 7.8866\n",
      "| Episode: 432 | Steps: 76 | Reward: -105.2103 | Qmax: 8.2103\n",
      "| Episode: 433 | Steps: 1601 | Reward: -181.0131 | Qmax: 8.2307\n",
      "| Episode: 434 | Steps: 80 | Reward: -104.1367 | Qmax: 8.2117\n",
      "| Episode: 435 | Steps: 74 | Reward: -102.4265 | Qmax: 8.1602\n",
      "| Episode: 436 | Steps: 86 | Reward: -103.6192 | Qmax: 8.2303\n",
      "| Episode: 437 | Steps: 1601 | Reward: -180.2723 | Qmax: 8.5913\n",
      "| Episode: 438 | Steps: 78 | Reward: -105.1923 | Qmax: 8.7243\n",
      "| Episode: 439 | Steps: 1601 | Reward: -182.5206 | Qmax: 8.8756\n",
      "| Episode: 440 | Steps: 1601 | Reward: -164.9781 | Qmax: 9.8269\n",
      "| Episode: 441 | Steps: 1601 | Reward: -159.4650 | Qmax: 10.2120\n",
      "| Episode: 442 | Steps: 1601 | Reward: -160.3032 | Qmax: 10.1672\n",
      "| Episode: 443 | Steps: 1601 | Reward: -143.5373 | Qmax: 10.2750\n",
      "| Episode: 444 | Steps: 1601 | Reward: -126.2708 | Qmax: 10.7642\n",
      "| Episode: 445 | Steps: 1601 | Reward: -134.3163 | Qmax: 10.5491\n",
      "| Episode: 446 | Steps: 1601 | Reward: -151.7864 | Qmax: 10.5349\n",
      "| Episode: 447 | Steps: 1601 | Reward: -152.4833 | Qmax: 10.4267\n",
      "| Episode: 448 | Steps: 1601 | Reward: -165.7503 | Qmax: 10.1953\n",
      "| Episode: 449 | Steps: 1601 | Reward: -185.0974 | Qmax: 10.0197\n",
      "| Episode: 450 | Steps: 1601 | Reward: -136.5728 | Qmax: 9.8354\n",
      "| Episode: 451 | Steps: 1601 | Reward: -164.5143 | Qmax: 9.6965\n",
      "| Episode: 452 | Steps: 1601 | Reward: -146.8378 | Qmax: 9.4487\n",
      "| Episode: 453 | Steps: 1601 | Reward: -162.8713 | Qmax: 9.3851\n",
      "| Episode: 454 | Steps: 1601 | Reward: -148.4349 | Qmax: 9.2463\n",
      "| Episode: 455 | Steps: 1601 | Reward: -173.8756 | Qmax: 9.6529\n",
      "| Episode: 456 | Steps: 1601 | Reward: -173.0710 | Qmax: 10.6915\n",
      "| Episode: 457 | Steps: 1601 | Reward: -177.0415 | Qmax: 11.9345\n",
      "| Episode: 458 | Steps: 1601 | Reward: -177.0699 | Qmax: 13.1919\n",
      "| Episode: 459 | Steps: 1601 | Reward: -177.6424 | Qmax: 14.2330\n",
      "| Episode: 460 | Steps: 1601 | Reward: -177.1261 | Qmax: 15.2843\n",
      "| Episode: 461 | Steps: 1601 | Reward: -176.1732 | Qmax: 16.0832\n",
      "| Episode: 462 | Steps: 1601 | Reward: -174.6252 | Qmax: 16.9106\n",
      "| Episode: 463 | Steps: 1601 | Reward: -177.6980 | Qmax: 17.6869\n",
      "| Episode: 464 | Steps: 1601 | Reward: -177.5703 | Qmax: 18.3381\n",
      "| Episode: 465 | Steps: 1601 | Reward: -176.6611 | Qmax: 18.8835\n",
      "| Episode: 466 | Steps: 1601 | Reward: -177.7840 | Qmax: 19.4160\n",
      "| Episode: 467 | Steps: 1601 | Reward: -166.8437 | Qmax: 19.6794\n",
      "| Episode: 468 | Steps: 1601 | Reward: -168.5398 | Qmax: 20.2006\n",
      "| Episode: 469 | Steps: 148 | Reward: -136.2840 | Qmax: 20.0972\n",
      "| Episode: 470 | Steps: 52 | Reward: -107.0336 | Qmax: 19.9855\n",
      "| Episode: 471 | Steps: 64 | Reward: -118.9272 | Qmax: 19.4601\n",
      "| Episode: 472 | Steps: 82 | Reward: -116.3284 | Qmax: 20.1479\n",
      "| Episode: 473 | Steps: 91 | Reward: -120.6390 | Qmax: 20.1275\n",
      "| Episode: 474 | Steps: 78 | Reward: -117.0811 | Qmax: 20.5654\n",
      "| Episode: 475 | Steps: 105 | Reward: -119.2020 | Qmax: 19.9504\n",
      "| Episode: 476 | Steps: 64 | Reward: -117.5480 | Qmax: 20.5023\n",
      "| Episode: 477 | Steps: 60 | Reward: -110.5101 | Qmax: 20.3416\n",
      "| Episode: 478 | Steps: 109 | Reward: -111.2264 | Qmax: 19.6320\n",
      "| Episode: 479 | Steps: 85 | Reward: -112.0379 | Qmax: 19.9324\n",
      "| Episode: 480 | Steps: 107 | Reward: -108.6688 | Qmax: 20.2111\n",
      "| Episode: 481 | Steps: 85 | Reward: -106.8452 | Qmax: 19.5309\n",
      "| Episode: 482 | Steps: 98 | Reward: -107.4202 | Qmax: 19.4945\n",
      "| Episode: 483 | Steps: 61 | Reward: -114.2018 | Qmax: 20.1770\n",
      "| Episode: 484 | Steps: 69 | Reward: -116.3058 | Qmax: 20.3713\n",
      "| Episode: 485 | Steps: 67 | Reward: -116.9277 | Qmax: 19.0020\n",
      "| Episode: 486 | Steps: 70 | Reward: -106.9492 | Qmax: 20.3584\n",
      "| Episode: 487 | Steps: 61 | Reward: -106.2611 | Qmax: 19.7435\n",
      "| Episode: 488 | Steps: 64 | Reward: -106.6713 | Qmax: 20.3356\n",
      "| Episode: 489 | Steps: 69 | Reward: -104.5139 | Qmax: 20.1126\n",
      "| Episode: 490 | Steps: 75 | Reward: -105.9831 | Qmax: 20.4762\n",
      "| Episode: 491 | Steps: 73 | Reward: -108.9328 | Qmax: 19.4001\n",
      "| Episode: 492 | Steps: 68 | Reward: -104.3335 | Qmax: 19.9221\n",
      "| Episode: 493 | Steps: 77 | Reward: -100.4727 | Qmax: 20.5313\n",
      "| Episode: 494 | Steps: 178 | Reward: -121.4768 | Qmax: 20.1894\n",
      "| Episode: 495 | Steps: 591 | Reward: -164.5045 | Qmax: 20.2628\n",
      "| Episode: 496 | Steps: 93 | Reward: -132.3349 | Qmax: 19.8940\n",
      "| Episode: 497 | Steps: 256 | Reward: -148.2217 | Qmax: 20.1797\n",
      "| Episode: 498 | Steps: 165 | Reward: -131.9495 | Qmax: 19.4344\n",
      "| Episode: 499 | Steps: 551 | Reward: -163.7976 | Qmax: 20.5283\n",
      "| Episode: 500 | Steps: 173 | Reward: -131.5854 | Qmax: 19.7140\n",
      "| Episode: 501 | Steps: 762 | Reward: -176.9597 | Qmax: 20.6665\n",
      "| Episode: 502 | Steps: 1601 | Reward: -178.4524 | Qmax: 20.3858\n",
      "| Episode: 503 | Steps: 1601 | Reward: -179.2156 | Qmax: 20.4670\n",
      "| Episode: 504 | Steps: 1601 | Reward: -175.4823 | Qmax: 20.5390\n",
      "| Episode: 505 | Steps: 1601 | Reward: -171.7164 | Qmax: 20.3136\n",
      "| Episode: 506 | Steps: 1601 | Reward: -161.6792 | Qmax: 20.0890\n",
      "| Episode: 507 | Steps: 1601 | Reward: -177.3126 | Qmax: 19.8864\n",
      "| Episode: 508 | Steps: 1601 | Reward: -176.2595 | Qmax: 19.6067\n",
      "| Episode: 509 | Steps: 1601 | Reward: -178.4222 | Qmax: 19.2952\n",
      "| Episode: 510 | Steps: 1601 | Reward: -173.1083 | Qmax: 18.9413\n",
      "| Episode: 511 | Steps: 1601 | Reward: -179.8482 | Qmax: 18.5719\n",
      "| Episode: 512 | Steps: 1601 | Reward: -175.0405 | Qmax: 18.2352\n",
      "| Episode: 513 | Steps: 1601 | Reward: -132.0520 | Qmax: 17.8514\n",
      "| Episode: 514 | Steps: 81 | Reward: -114.2935 | Qmax: 17.4857\n",
      "| Episode: 515 | Steps: 1601 | Reward: -146.6786 | Qmax: 17.5531\n",
      "| Episode: 516 | Steps: 1601 | Reward: -167.4075 | Qmax: 17.2118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 517 | Steps: 1601 | Reward: -169.2807 | Qmax: 16.9457\n",
      "| Episode: 518 | Steps: 1601 | Reward: -121.8341 | Qmax: 16.9477\n",
      "| Episode: 519 | Steps: 85 | Reward: -109.1956 | Qmax: 16.8182\n",
      "| Episode: 520 | Steps: 1601 | Reward: -140.9785 | Qmax: 16.7886\n",
      "| Episode: 521 | Steps: 62 | Reward: -106.2868 | Qmax: 16.1630\n",
      "| Episode: 522 | Steps: 71 | Reward: -105.1128 | Qmax: 16.2958\n",
      "| Episode: 523 | Steps: 1472 | Reward: -253.0995 | Qmax: 16.4695\n",
      "| Episode: 524 | Steps: 1601 | Reward: -168.8915 | Qmax: 16.1423\n",
      "| Episode: 525 | Steps: 150 | Reward: -137.2284 | Qmax: 15.8376\n",
      "| Episode: 526 | Steps: 1601 | Reward: -154.5413 | Qmax: 15.8245\n",
      "| Episode: 527 | Steps: 80 | Reward: -117.6581 | Qmax: 15.8537\n",
      "| Episode: 528 | Steps: 114 | Reward: -105.3345 | Qmax: 15.6894\n",
      "| Episode: 529 | Steps: 79 | Reward: -112.3687 | Qmax: 15.3679\n",
      "| Episode: 530 | Steps: 77 | Reward: -107.8580 | Qmax: 15.3629\n",
      "| Episode: 531 | Steps: 85 | Reward: -107.9214 | Qmax: 15.2640\n",
      "| Episode: 532 | Steps: 1601 | Reward: -150.3862 | Qmax: 15.5460\n",
      "| Episode: 533 | Steps: 1601 | Reward: -148.0986 | Qmax: 15.4344\n",
      "| Episode: 534 | Steps: 128 | Reward: -117.9503 | Qmax: 15.1335\n",
      "| Episode: 535 | Steps: 1601 | Reward: -138.2088 | Qmax: 15.2640\n",
      "| Episode: 536 | Steps: 605 | Reward: -176.2284 | Qmax: 15.1286\n",
      "| Episode: 537 | Steps: 1601 | Reward: -143.8334 | Qmax: 15.1159\n",
      "| Episode: 538 | Steps: 1601 | Reward: -130.2483 | Qmax: 15.0613\n",
      "| Episode: 539 | Steps: 1601 | Reward: -132.7379 | Qmax: 14.8958\n",
      "| Episode: 540 | Steps: 1601 | Reward: -144.0079 | Qmax: 14.9251\n",
      "| Episode: 541 | Steps: 1601 | Reward: -144.7055 | Qmax: 15.3869\n",
      "| Episode: 542 | Steps: 1601 | Reward: -154.5241 | Qmax: 15.9675\n",
      "| Episode: 543 | Steps: 1601 | Reward: -158.7989 | Qmax: 16.6999\n",
      "| Episode: 544 | Steps: 1601 | Reward: -153.3004 | Qmax: 17.1680\n",
      "| Episode: 545 | Steps: 1601 | Reward: -146.4781 | Qmax: 17.3211\n",
      "| Episode: 546 | Steps: 1601 | Reward: -153.4328 | Qmax: 17.6510\n",
      "| Episode: 547 | Steps: 1601 | Reward: -130.5109 | Qmax: 17.7736\n",
      "| Episode: 548 | Steps: 1601 | Reward: -124.9640 | Qmax: 17.9182\n",
      "| Episode: 549 | Steps: 1601 | Reward: -146.5809 | Qmax: 17.9856\n",
      "| Episode: 550 | Steps: 1601 | Reward: -146.2859 | Qmax: 17.4921\n",
      "| Episode: 551 | Steps: 1601 | Reward: -147.8273 | Qmax: 17.7585\n",
      "| Episode: 552 | Steps: 70 | Reward: -100.4936 | Qmax: 17.7090\n",
      "| Episode: 553 | Steps: 69 | Reward: -100.6077 | Qmax: 17.1691\n",
      "| Episode: 554 | Steps: 89 | Reward: -102.2577 | Qmax: 17.3392\n",
      "| Episode: 555 | Steps: 1601 | Reward: -145.7529 | Qmax: 17.7736\n",
      "| Episode: 556 | Steps: 1601 | Reward: -140.6708 | Qmax: 17.5081\n",
      "| Episode: 557 | Steps: 1601 | Reward: -142.0602 | Qmax: 17.4162\n",
      "| Episode: 558 | Steps: 117 | Reward: -105.2230 | Qmax: 16.8498\n",
      "| Episode: 559 | Steps: 72 | Reward: -98.6160 | Qmax: 17.3936\n",
      "| Episode: 560 | Steps: 1385 | Reward: -254.4433 | Qmax: 17.1751\n",
      "| Episode: 561 | Steps: 90 | Reward: -111.4097 | Qmax: 17.4680\n",
      "| Episode: 562 | Steps: 52 | Reward: -107.7124 | Qmax: 17.3031\n",
      "| Episode: 563 | Steps: 70 | Reward: -101.4500 | Qmax: 17.1138\n",
      "| Episode: 564 | Steps: 88 | Reward: -92.2906 | Qmax: 16.9098\n",
      "| Episode: 565 | Steps: 126 | Reward: -100.1416 | Qmax: 17.3639\n",
      "| Episode: 566 | Steps: 79 | Reward: -116.8092 | Qmax: 16.2963\n",
      "| Episode: 567 | Steps: 73 | Reward: -106.7631 | Qmax: 16.8204\n",
      "| Episode: 568 | Steps: 67 | Reward: -101.3909 | Qmax: 16.3812\n",
      "| Episode: 569 | Steps: 84 | Reward: -117.9359 | Qmax: 16.3641\n",
      "| Episode: 570 | Steps: 82 | Reward: -99.3238 | Qmax: 16.7841\n",
      "| Episode: 571 | Steps: 1221 | Reward: -241.1747 | Qmax: 16.7350\n",
      "| Episode: 572 | Steps: 1601 | Reward: -120.8597 | Qmax: 16.7119\n",
      "| Episode: 573 | Steps: 139 | Reward: -104.8165 | Qmax: 16.4139\n",
      "| Episode: 574 | Steps: 119 | Reward: -106.7966 | Qmax: 16.3606\n",
      "| Episode: 575 | Steps: 109 | Reward: -100.2088 | Qmax: 16.8565\n",
      "| Episode: 576 | Steps: 120 | Reward: -112.5846 | Qmax: 16.2891\n",
      "| Episode: 577 | Steps: 144 | Reward: -98.5890 | Qmax: 16.4039\n",
      "| Episode: 578 | Steps: 109 | Reward: -104.3802 | Qmax: 16.2516\n",
      "| Episode: 579 | Steps: 157 | Reward: -95.3467 | Qmax: 16.5758\n",
      "| Episode: 580 | Steps: 134 | Reward: -100.1828 | Qmax: 16.1373\n",
      "| Episode: 581 | Steps: 154 | Reward: -105.8733 | Qmax: 16.1948\n",
      "| Episode: 582 | Steps: 135 | Reward: -101.7826 | Qmax: 16.4733\n",
      "| Episode: 583 | Steps: 1601 | Reward: -159.6968 | Qmax: 16.2849\n",
      "| Episode: 584 | Steps: 156 | Reward: -107.9753 | Qmax: 15.8500\n",
      "| Episode: 585 | Steps: 83 | Reward: -102.8616 | Qmax: 16.5986\n",
      "| Episode: 586 | Steps: 1601 | Reward: -127.0049 | Qmax: 16.1257\n",
      "| Episode: 587 | Steps: 1601 | Reward: -131.7783 | Qmax: 16.1009\n",
      "| Episode: 588 | Steps: 111 | Reward: -103.7855 | Qmax: 15.5632\n",
      "| Episode: 589 | Steps: 1601 | Reward: -121.3770 | Qmax: 15.5718\n",
      "| Episode: 590 | Steps: 265 | Reward: -111.2382 | Qmax: 15.3288\n",
      "| Episode: 591 | Steps: 1601 | Reward: -144.3025 | Qmax: 15.0368\n",
      "| Episode: 592 | Steps: 169 | Reward: -103.3249 | Qmax: 15.3374\n",
      "| Episode: 593 | Steps: 1601 | Reward: -161.9351 | Qmax: 14.9889\n",
      "| Episode: 594 | Steps: 684 | Reward: -181.0448 | Qmax: 15.0495\n",
      "| Episode: 595 | Steps: 1601 | Reward: -173.8217 | Qmax: 14.6106\n",
      "| Episode: 596 | Steps: 1601 | Reward: -158.4658 | Qmax: 14.4074\n",
      "| Episode: 597 | Steps: 249 | Reward: -99.9233 | Qmax: 14.3202\n",
      "| Episode: 598 | Steps: 254 | Reward: -113.4984 | Qmax: 14.1693\n",
      "| Episode: 599 | Steps: 1601 | Reward: -160.2859 | Qmax: 14.2347\n",
      "| Episode: 600 | Steps: 1601 | Reward: -156.4960 | Qmax: 13.9804\n",
      "| Episode: 601 | Steps: 1601 | Reward: -168.1803 | Qmax: 13.9312\n",
      "| Episode: 602 | Steps: 1601 | Reward: -160.3708 | Qmax: 13.5876\n",
      "| Episode: 603 | Steps: 1601 | Reward: -154.7367 | Qmax: 13.4568\n",
      "| Episode: 604 | Steps: 1086 | Reward: -223.7977 | Qmax: 13.3874\n",
      "| Episode: 605 | Steps: 881 | Reward: -199.1910 | Qmax: 12.9285\n",
      "| Episode: 606 | Steps: 994 | Reward: -201.5001 | Qmax: 12.6846\n",
      "| Episode: 607 | Steps: 1197 | Reward: -233.5253 | Qmax: 12.3615\n",
      "| Episode: 608 | Steps: 1048 | Reward: -208.9545 | Qmax: 12.1775\n",
      "| Episode: 609 | Steps: 1252 | Reward: -189.6382 | Qmax: 12.0561\n",
      "| Episode: 610 | Steps: 143 | Reward: -117.7417 | Qmax: 11.9179\n",
      "| Episode: 611 | Steps: 111 | Reward: -120.5257 | Qmax: 11.8037\n",
      "| Episode: 612 | Steps: 1291 | Reward: -223.6893 | Qmax: 11.6985\n",
      "| Episode: 613 | Steps: 149 | Reward: -128.5474 | Qmax: 11.5949\n",
      "| Episode: 614 | Steps: 1218 | Reward: -233.4262 | Qmax: 11.6488\n",
      "| Episode: 615 | Steps: 1601 | Reward: -156.4540 | Qmax: 11.3706\n",
      "| Episode: 616 | Steps: 332 | Reward: -120.8533 | Qmax: 10.8970\n",
      "| Episode: 617 | Steps: 223 | Reward: -118.8119 | Qmax: 11.0968\n",
      "| Episode: 618 | Steps: 137 | Reward: -109.6973 | Qmax: 10.6607\n",
      "| Episode: 619 | Steps: 1025 | Reward: -182.0178 | Qmax: 10.9183\n",
      "| Episode: 620 | Steps: 157 | Reward: -111.6398 | Qmax: 10.4574\n",
      "| Episode: 621 | Steps: 1601 | Reward: -176.6874 | Qmax: 10.5393\n",
      "| Episode: 622 | Steps: 1601 | Reward: -173.5084 | Qmax: 10.4023\n",
      "| Episode: 623 | Steps: 113 | Reward: -124.4542 | Qmax: 10.0427\n",
      "| Episode: 624 | Steps: 115 | Reward: -120.5946 | Qmax: 10.0551\n",
      "| Episode: 625 | Steps: 140 | Reward: -121.4239 | Qmax: 10.1250\n",
      "| Episode: 626 | Steps: 173 | Reward: -122.2801 | Qmax: 9.8650\n",
      "| Episode: 627 | Steps: 472 | Reward: -166.7780 | Qmax: 10.2619\n",
      "| Episode: 628 | Steps: 156 | Reward: -121.8680 | Qmax: 9.9932\n",
      "| Episode: 629 | Steps: 244 | Reward: -127.4950 | Qmax: 9.7888\n",
      "| Episode: 630 | Steps: 744 | Reward: -184.9114 | Qmax: 9.8105\n",
      "| Episode: 631 | Steps: 140 | Reward: -115.2476 | Qmax: 9.6302\n",
      "| Episode: 632 | Steps: 1601 | Reward: -174.9003 | Qmax: 9.5749\n",
      "| Episode: 633 | Steps: 134 | Reward: -126.0382 | Qmax: 9.5537\n",
      "| Episode: 634 | Steps: 158 | Reward: -129.5414 | Qmax: 9.4906\n",
      "| Episode: 635 | Steps: 135 | Reward: -123.1324 | Qmax: 9.5639\n",
      "| Episode: 636 | Steps: 230 | Reward: -132.0383 | Qmax: 9.2006\n",
      "| Episode: 637 | Steps: 126 | Reward: -127.0303 | Qmax: 9.3108\n",
      "| Episode: 638 | Steps: 603 | Reward: -147.8551 | Qmax: 9.3258\n",
      "| Episode: 639 | Steps: 302 | Reward: -122.3949 | Qmax: 9.2240\n",
      "| Episode: 640 | Steps: 550 | Reward: -146.1178 | Qmax: 9.1777\n",
      "| Episode: 641 | Steps: 707 | Reward: -158.0883 | Qmax: 8.9460\n",
      "| Episode: 642 | Steps: 282 | Reward: -131.5294 | Qmax: 8.8961\n",
      "| Episode: 643 | Steps: 192 | Reward: -123.6662 | Qmax: 8.8377\n",
      "| Episode: 644 | Steps: 209 | Reward: -128.7729 | Qmax: 8.8050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 645 | Steps: 116 | Reward: -120.3814 | Qmax: 8.6338\n",
      "| Episode: 646 | Steps: 177 | Reward: -126.2246 | Qmax: 8.7796\n",
      "| Episode: 647 | Steps: 145 | Reward: -122.3985 | Qmax: 8.4920\n",
      "| Episode: 648 | Steps: 299 | Reward: -130.2782 | Qmax: 8.7090\n",
      "| Episode: 649 | Steps: 169 | Reward: -114.7723 | Qmax: 8.4760\n",
      "| Episode: 650 | Steps: 204 | Reward: -112.2127 | Qmax: 8.5769\n",
      "| Episode: 651 | Steps: 206 | Reward: -104.3613 | Qmax: 8.5511\n",
      "| Episode: 652 | Steps: 142 | Reward: -119.2319 | Qmax: 8.4212\n",
      "| Episode: 653 | Steps: 126 | Reward: -122.2957 | Qmax: 8.4577\n",
      "| Episode: 654 | Steps: 111 | Reward: -111.1989 | Qmax: 8.7014\n",
      "| Episode: 655 | Steps: 153 | Reward: -126.5629 | Qmax: 8.6109\n",
      "| Episode: 656 | Steps: 125 | Reward: -120.5978 | Qmax: 8.5238\n",
      "| Episode: 657 | Steps: 116 | Reward: -117.5440 | Qmax: 8.4980\n",
      "| Episode: 658 | Steps: 439 | Reward: -134.1591 | Qmax: 8.5855\n",
      "| Episode: 659 | Steps: 217 | Reward: -127.6139 | Qmax: 8.5078\n",
      "| Episode: 660 | Steps: 124 | Reward: -120.4552 | Qmax: 8.4729\n",
      "| Episode: 661 | Steps: 106 | Reward: -109.1384 | Qmax: 8.2446\n",
      "| Episode: 662 | Steps: 185 | Reward: -126.0315 | Qmax: 8.2892\n",
      "| Episode: 663 | Steps: 159 | Reward: -103.2450 | Qmax: 8.2673\n",
      "| Episode: 664 | Steps: 183 | Reward: -121.2736 | Qmax: 8.2117\n",
      "| Episode: 665 | Steps: 188 | Reward: -112.6817 | Qmax: 8.2818\n",
      "| Episode: 666 | Steps: 250 | Reward: -114.0150 | Qmax: 8.3798\n",
      "| Episode: 667 | Steps: 355 | Reward: -117.6814 | Qmax: 8.2820\n",
      "| Episode: 668 | Steps: 149 | Reward: -126.9383 | Qmax: 8.1211\n",
      "| Episode: 669 | Steps: 296 | Reward: -112.6223 | Qmax: 8.2464\n",
      "| Episode: 670 | Steps: 462 | Reward: -125.4267 | Qmax: 8.2256\n",
      "| Episode: 671 | Steps: 265 | Reward: -106.6040 | Qmax: 8.1136\n",
      "| Episode: 672 | Steps: 237 | Reward: -110.6868 | Qmax: 8.1944\n",
      "| Episode: 673 | Steps: 187 | Reward: -124.9962 | Qmax: 8.0915\n",
      "| Episode: 674 | Steps: 114 | Reward: -126.9688 | Qmax: 8.2329\n",
      "| Episode: 675 | Steps: 214 | Reward: -129.1824 | Qmax: 8.1143\n",
      "| Episode: 676 | Steps: 518 | Reward: -167.2362 | Qmax: 8.1485\n",
      "| Episode: 677 | Steps: 534 | Reward: -156.0805 | Qmax: 8.0978\n",
      "| Episode: 678 | Steps: 180 | Reward: -122.2782 | Qmax: 8.0711\n",
      "| Episode: 679 | Steps: 327 | Reward: -108.6304 | Qmax: 8.0970\n",
      "| Episode: 680 | Steps: 221 | Reward: -100.3883 | Qmax: 7.8302\n",
      "| Episode: 681 | Steps: 170 | Reward: -96.5100 | Qmax: 7.8977\n",
      "| Episode: 682 | Steps: 1601 | Reward: -148.1695 | Qmax: 7.9005\n",
      "| Episode: 683 | Steps: 148 | Reward: -91.8976 | Qmax: 7.8804\n",
      "| Episode: 684 | Steps: 167 | Reward: -96.8354 | Qmax: 7.8849\n",
      "| Episode: 685 | Steps: 138 | Reward: -95.8265 | Qmax: 7.6999\n",
      "| Episode: 686 | Steps: 492 | Reward: -159.1461 | Qmax: 7.6519\n",
      "| Episode: 687 | Steps: 113 | Reward: -97.9203 | Qmax: 7.6566\n",
      "| Episode: 688 | Steps: 229 | Reward: -101.4341 | Qmax: 7.6971\n",
      "| Episode: 689 | Steps: 208 | Reward: -102.0471 | Qmax: 7.7190\n",
      "| Episode: 690 | Steps: 183 | Reward: -106.2821 | Qmax: 7.5731\n",
      "| Episode: 691 | Steps: 1601 | Reward: -123.5322 | Qmax: 7.5912\n",
      "| Episode: 692 | Steps: 189 | Reward: -96.3246 | Qmax: 7.4984\n",
      "| Episode: 693 | Steps: 134 | Reward: -91.8489 | Qmax: 7.4015\n",
      "| Episode: 694 | Steps: 175 | Reward: -88.5353 | Qmax: 7.4667\n",
      "| Episode: 695 | Steps: 113 | Reward: -102.5456 | Qmax: 7.3452\n",
      "| Episode: 696 | Steps: 160 | Reward: -110.8517 | Qmax: 7.5057\n",
      "| Episode: 697 | Steps: 368 | Reward: -152.7265 | Qmax: 7.5464\n",
      "| Episode: 698 | Steps: 199 | Reward: -99.4846 | Qmax: 7.4990\n",
      "| Episode: 699 | Steps: 477 | Reward: -162.6720 | Qmax: 7.4018\n",
      "| Episode: 700 | Steps: 168 | Reward: -93.7356 | Qmax: 7.4805\n",
      "| Episode: 701 | Steps: 147 | Reward: -100.3638 | Qmax: 7.4631\n",
      "| Episode: 702 | Steps: 640 | Reward: -185.1526 | Qmax: 7.3338\n",
      "| Episode: 703 | Steps: 256 | Reward: -147.8989 | Qmax: 7.2997\n",
      "| Episode: 704 | Steps: 154 | Reward: -101.8410 | Qmax: 7.3318\n",
      "| Episode: 705 | Steps: 1601 | Reward: -101.6466 | Qmax: 7.3271\n",
      "| Episode: 706 | Steps: 188 | Reward: -98.9233 | Qmax: 7.3414\n",
      "| Episode: 707 | Steps: 803 | Reward: -196.0487 | Qmax: 7.2909\n",
      "| Episode: 708 | Steps: 140 | Reward: -107.2763 | Qmax: 7.2519\n",
      "| Episode: 709 | Steps: 132 | Reward: -100.0841 | Qmax: 7.1985\n",
      "| Episode: 710 | Steps: 175 | Reward: -99.3513 | Qmax: 7.2167\n",
      "| Episode: 711 | Steps: 320 | Reward: -149.1707 | Qmax: 7.1109\n",
      "| Episode: 712 | Steps: 140 | Reward: -99.3132 | Qmax: 7.2085\n",
      "| Episode: 713 | Steps: 196 | Reward: -106.6322 | Qmax: 7.3203\n",
      "| Episode: 714 | Steps: 210 | Reward: -88.1638 | Qmax: 7.0319\n",
      "| Episode: 715 | Steps: 148 | Reward: -104.3948 | Qmax: 7.1069\n",
      "| Episode: 716 | Steps: 142 | Reward: -108.5095 | Qmax: 7.1857\n",
      "| Episode: 717 | Steps: 1601 | Reward: -87.1680 | Qmax: 7.1407\n",
      "| Episode: 718 | Steps: 229 | Reward: -127.5637 | Qmax: 7.0354\n",
      "| Episode: 719 | Steps: 1601 | Reward: -142.8777 | Qmax: 6.9924\n",
      "| Episode: 720 | Steps: 1601 | Reward: -129.6074 | Qmax: 6.9125\n",
      "| Episode: 721 | Steps: 1601 | Reward: -118.8766 | Qmax: 6.8664\n",
      "| Episode: 722 | Steps: 285 | Reward: -126.2332 | Qmax: 6.8353\n",
      "| Episode: 723 | Steps: 177 | Reward: -136.5739 | Qmax: 6.8535\n",
      "| Episode: 724 | Steps: 396 | Reward: -163.7171 | Qmax: 6.8150\n",
      "| Episode: 725 | Steps: 652 | Reward: -173.7323 | Qmax: 6.7934\n",
      "| Episode: 726 | Steps: 366 | Reward: -153.5181 | Qmax: 6.7766\n",
      "| Episode: 727 | Steps: 169 | Reward: -143.1468 | Qmax: 6.5917\n",
      "| Episode: 728 | Steps: 1601 | Reward: -139.9686 | Qmax: 6.7081\n",
      "| Episode: 729 | Steps: 138 | Reward: -102.1855 | Qmax: 6.4041\n",
      "| Episode: 730 | Steps: 255 | Reward: -141.6786 | Qmax: 6.4913\n",
      "| Episode: 731 | Steps: 117 | Reward: -102.7809 | Qmax: 6.4257\n",
      "| Episode: 732 | Steps: 211 | Reward: -149.9447 | Qmax: 6.3330\n",
      "| Episode: 733 | Steps: 205 | Reward: -139.9414 | Qmax: 6.4686\n",
      "| Episode: 734 | Steps: 345 | Reward: -126.0395 | Qmax: 6.5151\n",
      "| Episode: 735 | Steps: 167 | Reward: -145.7238 | Qmax: 6.2518\n",
      "| Episode: 736 | Steps: 1601 | Reward: -144.4752 | Qmax: 6.4356\n",
      "| Episode: 737 | Steps: 636 | Reward: -190.6716 | Qmax: 6.4364\n",
      "| Episode: 738 | Steps: 1601 | Reward: -144.4210 | Qmax: 6.2760\n",
      "| Episode: 739 | Steps: 295 | Reward: -148.4121 | Qmax: 6.2514\n",
      "| Episode: 740 | Steps: 360 | Reward: -155.1275 | Qmax: 6.1708\n",
      "| Episode: 741 | Steps: 1601 | Reward: -157.4327 | Qmax: 6.1265\n",
      "| Episode: 742 | Steps: 1601 | Reward: -159.1584 | Qmax: 6.1981\n",
      "| Episode: 743 | Steps: 1601 | Reward: -141.2476 | Qmax: 6.0854\n",
      "| Episode: 744 | Steps: 1503 | Reward: -217.2736 | Qmax: 6.0410\n",
      "| Episode: 745 | Steps: 216 | Reward: -108.9172 | Qmax: 5.8908\n",
      "| Episode: 746 | Steps: 406 | Reward: -124.1378 | Qmax: 5.8778\n",
      "| Episode: 747 | Steps: 951 | Reward: -199.3938 | Qmax: 5.9359\n",
      "| Episode: 748 | Steps: 331 | Reward: -115.6142 | Qmax: 5.8335\n",
      "| Episode: 749 | Steps: 348 | Reward: -118.9688 | Qmax: 5.8065\n",
      "| Episode: 750 | Steps: 1601 | Reward: -121.4130 | Qmax: 5.7332\n",
      "| Episode: 751 | Steps: 199 | Reward: -119.3026 | Qmax: 5.6678\n",
      "| Episode: 752 | Steps: 112 | Reward: -108.3757 | Qmax: 5.5373\n",
      "| Episode: 753 | Steps: 87 | Reward: -105.3699 | Qmax: 5.6675\n",
      "| Episode: 754 | Steps: 217 | Reward: -105.7901 | Qmax: 5.6426\n",
      "| Episode: 755 | Steps: 1601 | Reward: -151.5477 | Qmax: 5.7702\n",
      "| Episode: 756 | Steps: 200 | Reward: -108.6542 | Qmax: 5.7386\n",
      "| Episode: 757 | Steps: 538 | Reward: -138.2089 | Qmax: 5.7804\n",
      "| Episode: 758 | Steps: 284 | Reward: -120.0994 | Qmax: 5.8467\n",
      "| Episode: 759 | Steps: 1601 | Reward: -171.3327 | Qmax: 5.8095\n",
      "| Episode: 760 | Steps: 283 | Reward: -133.1752 | Qmax: 5.8021\n",
      "| Episode: 761 | Steps: 1601 | Reward: -159.2455 | Qmax: 5.7617\n",
      "| Episode: 762 | Steps: 173 | Reward: -101.3450 | Qmax: 5.8636\n",
      "| Episode: 763 | Steps: 1322 | Reward: -183.5109 | Qmax: 5.7757\n",
      "| Episode: 764 | Steps: 1601 | Reward: -104.2678 | Qmax: 5.7491\n",
      "| Episode: 765 | Steps: 172 | Reward: -104.8053 | Qmax: 5.7056\n",
      "| Episode: 766 | Steps: 1601 | Reward: -104.7912 | Qmax: 5.6729\n",
      "| Episode: 767 | Steps: 601 | Reward: -127.4199 | Qmax: 5.6720\n",
      "| Episode: 768 | Steps: 352 | Reward: -109.0807 | Qmax: 5.7094\n",
      "| Episode: 769 | Steps: 1501 | Reward: -260.4162 | Qmax: 5.6700\n",
      "| Episode: 770 | Steps: 1601 | Reward: -159.4891 | Qmax: 5.6430\n",
      "| Episode: 771 | Steps: 232 | Reward: -115.0310 | Qmax: 5.6713\n",
      "| Episode: 772 | Steps: 309 | Reward: -111.5905 | Qmax: 5.6377\n",
      "| Episode: 773 | Steps: 540 | Reward: -132.5204 | Qmax: 5.6053\n",
      "| Episode: 774 | Steps: 107 | Reward: -98.3495 | Qmax: 5.6265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 775 | Steps: 158 | Reward: -101.9807 | Qmax: 5.5996\n",
      "| Episode: 776 | Steps: 1601 | Reward: -137.5676 | Qmax: 5.5875\n",
      "| Episode: 777 | Steps: 114 | Reward: -96.0860 | Qmax: 5.4737\n",
      "| Episode: 778 | Steps: 218 | Reward: -105.7592 | Qmax: 5.5305\n",
      "| Episode: 779 | Steps: 861 | Reward: -157.9087 | Qmax: 5.5741\n",
      "| Episode: 780 | Steps: 62 | Reward: -120.2164 | Qmax: 5.5325\n",
      "| Episode: 781 | Steps: 173 | Reward: -105.1071 | Qmax: 5.5596\n",
      "| Episode: 782 | Steps: 177 | Reward: -100.3615 | Qmax: 5.5796\n",
      "| Episode: 783 | Steps: 103 | Reward: -101.8622 | Qmax: 5.6596\n",
      "| Episode: 784 | Steps: 205 | Reward: -101.2226 | Qmax: 5.4086\n",
      "| Episode: 785 | Steps: 307 | Reward: -113.1380 | Qmax: 5.5296\n",
      "| Episode: 786 | Steps: 264 | Reward: -109.0670 | Qmax: 5.5467\n",
      "| Episode: 787 | Steps: 252 | Reward: -116.2542 | Qmax: 5.3874\n",
      "| Episode: 788 | Steps: 1601 | Reward: -147.4287 | Qmax: 5.5251\n",
      "| Episode: 789 | Steps: 143 | Reward: -99.6618 | Qmax: 5.6045\n",
      "| Episode: 790 | Steps: 1601 | Reward: -153.3104 | Qmax: 5.6040\n",
      "| Episode: 791 | Steps: 1601 | Reward: -145.9395 | Qmax: 5.7075\n",
      "| Episode: 792 | Steps: 1601 | Reward: -131.6544 | Qmax: 5.7308\n",
      "| Episode: 793 | Steps: 137 | Reward: -116.3576 | Qmax: 5.7819\n",
      "| Episode: 794 | Steps: 140 | Reward: -105.1890 | Qmax: 5.6837\n",
      "| Episode: 795 | Steps: 1601 | Reward: -141.4723 | Qmax: 5.6729\n",
      "| Episode: 796 | Steps: 57 | Reward: -115.0474 | Qmax: 5.5780\n",
      "| Episode: 797 | Steps: 1601 | Reward: -141.9470 | Qmax: 5.7068\n",
      "| Episode: 798 | Steps: 1601 | Reward: -131.4607 | Qmax: 5.5869\n",
      "| Episode: 799 | Steps: 1601 | Reward: -147.3070 | Qmax: 5.5852\n",
      "| Episode: 800 | Steps: 438 | Reward: -114.4592 | Qmax: 5.5094\n",
      "| Episode: 801 | Steps: 565 | Reward: -124.2471 | Qmax: 5.4572\n",
      "| Episode: 802 | Steps: 1601 | Reward: -136.3952 | Qmax: 5.4632\n",
      "| Episode: 803 | Steps: 1601 | Reward: -133.1833 | Qmax: 5.5568\n",
      "| Episode: 804 | Steps: 1601 | Reward: -120.9130 | Qmax: 5.4416\n",
      "| Episode: 805 | Steps: 149 | Reward: -101.0345 | Qmax: 5.3010\n",
      "| Episode: 806 | Steps: 282 | Reward: -106.6432 | Qmax: 5.4601\n",
      "| Episode: 807 | Steps: 1601 | Reward: -123.3026 | Qmax: 5.4371\n",
      "| Episode: 808 | Steps: 1601 | Reward: -122.1380 | Qmax: 5.3302\n",
      "| Episode: 809 | Steps: 424 | Reward: -121.0035 | Qmax: 5.3678\n",
      "| Episode: 810 | Steps: 1279 | Reward: -209.5214 | Qmax: 5.4294\n",
      "| Episode: 811 | Steps: 1601 | Reward: -142.4509 | Qmax: 5.3926\n",
      "| Episode: 812 | Steps: 1601 | Reward: -134.5226 | Qmax: 5.4217\n",
      "| Episode: 813 | Steps: 1601 | Reward: -134.3916 | Qmax: 5.3717\n",
      "| Episode: 814 | Steps: 222 | Reward: -104.0236 | Qmax: 5.2944\n",
      "| Episode: 815 | Steps: 1601 | Reward: -146.8350 | Qmax: 5.3253\n",
      "| Episode: 816 | Steps: 55 | Reward: -114.7102 | Qmax: 5.2447\n",
      "| Episode: 817 | Steps: 1601 | Reward: -139.1947 | Qmax: 5.3236\n",
      "| Episode: 818 | Steps: 91 | Reward: -100.0024 | Qmax: 5.0501\n",
      "| Episode: 819 | Steps: 1601 | Reward: -132.1701 | Qmax: 5.1742\n",
      "| Episode: 820 | Steps: 575 | Reward: -136.8517 | Qmax: 5.1286\n",
      "| Episode: 821 | Steps: 1601 | Reward: -152.9427 | Qmax: 5.1767\n",
      "| Episode: 822 | Steps: 121 | Reward: -104.7368 | Qmax: 5.0269\n",
      "| Episode: 823 | Steps: 1601 | Reward: -129.8246 | Qmax: 5.1736\n",
      "| Episode: 824 | Steps: 1601 | Reward: -141.4158 | Qmax: 5.3306\n",
      "| Episode: 825 | Steps: 1541 | Reward: -289.0839 | Qmax: 5.3569\n",
      "| Episode: 826 | Steps: 1601 | Reward: -165.4257 | Qmax: 5.3797\n",
      "| Episode: 827 | Steps: 1601 | Reward: -142.7081 | Qmax: 5.4624\n",
      "| Episode: 828 | Steps: 1527 | Reward: -222.6557 | Qmax: 5.5167\n",
      "| Episode: 829 | Steps: 1339 | Reward: -219.7242 | Qmax: 5.5456\n",
      "| Episode: 830 | Steps: 1601 | Reward: -149.8360 | Qmax: 5.6385\n",
      "| Episode: 831 | Steps: 1425 | Reward: -210.2388 | Qmax: 5.8238\n",
      "| Episode: 832 | Steps: 154 | Reward: -136.0285 | Qmax: 5.8618\n",
      "| Episode: 833 | Steps: 267 | Reward: -113.6971 | Qmax: 6.1154\n",
      "| Episode: 834 | Steps: 835 | Reward: -165.5952 | Qmax: 6.1789\n",
      "| Episode: 835 | Steps: 271 | Reward: -120.5211 | Qmax: 6.3568\n",
      "| Episode: 836 | Steps: 513 | Reward: -157.0926 | Qmax: 6.3242\n",
      "| Episode: 837 | Steps: 471 | Reward: -142.6770 | Qmax: 6.5289\n",
      "| Episode: 838 | Steps: 492 | Reward: -170.1350 | Qmax: 6.6316\n",
      "| Episode: 839 | Steps: 687 | Reward: -140.4827 | Qmax: 6.7641\n",
      "| Episode: 840 | Steps: 261 | Reward: -137.3604 | Qmax: 7.0362\n",
      "| Episode: 841 | Steps: 420 | Reward: -161.8828 | Qmax: 6.9697\n",
      "| Episode: 842 | Steps: 585 | Reward: -150.8191 | Qmax: 7.0071\n",
      "| Episode: 843 | Steps: 69 | Reward: -115.2903 | Qmax: 7.2578\n",
      "| Episode: 844 | Steps: 142 | Reward: -117.4852 | Qmax: 7.2031\n",
      "| Episode: 845 | Steps: 1601 | Reward: -164.2175 | Qmax: 7.3116\n",
      "| Episode: 846 | Steps: 469 | Reward: -169.8235 | Qmax: 7.3823\n",
      "| Episode: 847 | Steps: 148 | Reward: -124.7741 | Qmax: 7.7417\n",
      "| Episode: 848 | Steps: 129 | Reward: -104.8680 | Qmax: 7.3215\n",
      "| Episode: 849 | Steps: 427 | Reward: -163.3760 | Qmax: 7.5415\n",
      "| Episode: 850 | Steps: 1601 | Reward: -156.2271 | Qmax: 7.6655\n",
      "| Episode: 851 | Steps: 755 | Reward: -184.4035 | Qmax: 7.7025\n",
      "| Episode: 852 | Steps: 1162 | Reward: -186.4094 | Qmax: 7.8744\n",
      "| Episode: 853 | Steps: 657 | Reward: -120.6778 | Qmax: 7.8303\n",
      "| Episode: 854 | Steps: 509 | Reward: -155.3998 | Qmax: 7.8838\n",
      "| Episode: 855 | Steps: 136 | Reward: -108.9195 | Qmax: 7.7688\n",
      "| Episode: 856 | Steps: 137 | Reward: -103.0503 | Qmax: 7.7788\n",
      "| Episode: 857 | Steps: 416 | Reward: -162.5719 | Qmax: 7.8607\n",
      "| Episode: 858 | Steps: 969 | Reward: -180.0162 | Qmax: 7.7566\n",
      "| Episode: 859 | Steps: 164 | Reward: -119.5913 | Qmax: 7.5608\n",
      "| Episode: 860 | Steps: 187 | Reward: -106.1828 | Qmax: 7.5424\n",
      "| Episode: 861 | Steps: 1601 | Reward: -170.3602 | Qmax: 7.7878\n",
      "| Episode: 862 | Steps: 311 | Reward: -126.0583 | Qmax: 7.5610\n",
      "| Episode: 863 | Steps: 433 | Reward: -144.3795 | Qmax: 7.6116\n",
      "| Episode: 864 | Steps: 1179 | Reward: -202.3183 | Qmax: 7.5188\n",
      "| Episode: 865 | Steps: 507 | Reward: -161.3941 | Qmax: 7.5265\n",
      "| Episode: 866 | Steps: 477 | Reward: -156.9508 | Qmax: 7.5430\n",
      "| Episode: 867 | Steps: 155 | Reward: -127.6890 | Qmax: 7.3074\n",
      "| Episode: 868 | Steps: 332 | Reward: -114.5999 | Qmax: 7.3558\n",
      "| Episode: 869 | Steps: 237 | Reward: -94.6095 | Qmax: 7.3896\n",
      "| Episode: 870 | Steps: 1601 | Reward: -152.5979 | Qmax: 7.2775\n",
      "| Episode: 871 | Steps: 1601 | Reward: -151.0285 | Qmax: 7.2495\n",
      "| Episode: 872 | Steps: 1601 | Reward: -156.6963 | Qmax: 7.0520\n",
      "| Episode: 873 | Steps: 1111 | Reward: -208.7883 | Qmax: 7.0617\n",
      "| Episode: 874 | Steps: 1601 | Reward: -167.7222 | Qmax: 6.9800\n",
      "| Episode: 875 | Steps: 1601 | Reward: -152.6445 | Qmax: 6.8501\n",
      "| Episode: 876 | Steps: 1601 | Reward: -167.0431 | Qmax: 6.7069\n",
      "| Episode: 877 | Steps: 1601 | Reward: -172.3811 | Qmax: 6.5383\n",
      "| Episode: 878 | Steps: 1601 | Reward: -165.0031 | Qmax: 6.4998\n",
      "| Episode: 879 | Steps: 1601 | Reward: -133.4461 | Qmax: 6.3273\n",
      "| Episode: 880 | Steps: 1601 | Reward: -159.1155 | Qmax: 6.0859\n",
      "| Episode: 881 | Steps: 1601 | Reward: -115.1710 | Qmax: 6.1088\n",
      "| Episode: 882 | Steps: 1601 | Reward: -120.4860 | Qmax: 6.0143\n",
      "| Episode: 883 | Steps: 439 | Reward: -115.8232 | Qmax: 5.9206\n",
      "| Episode: 884 | Steps: 154 | Reward: -102.4080 | Qmax: 6.0351\n",
      "| Episode: 885 | Steps: 1601 | Reward: -150.7354 | Qmax: 5.8933\n",
      "| Episode: 886 | Steps: 240 | Reward: -105.4819 | Qmax: 5.9046\n",
      "| Episode: 887 | Steps: 146 | Reward: -101.5820 | Qmax: 5.8057\n",
      "| Episode: 888 | Steps: 595 | Reward: -125.6414 | Qmax: 5.7461\n",
      "| Episode: 889 | Steps: 641 | Reward: -119.3196 | Qmax: 5.8209\n",
      "| Episode: 890 | Steps: 1601 | Reward: -150.4469 | Qmax: 5.6410\n",
      "| Episode: 891 | Steps: 133 | Reward: -99.1536 | Qmax: 5.6248\n",
      "| Episode: 892 | Steps: 192 | Reward: -96.5176 | Qmax: 5.5195\n",
      "| Episode: 893 | Steps: 1601 | Reward: -140.6335 | Qmax: 5.6118\n",
      "| Episode: 894 | Steps: 337 | Reward: -105.7481 | Qmax: 5.4401\n",
      "| Episode: 895 | Steps: 395 | Reward: -116.3230 | Qmax: 5.4771\n",
      "| Episode: 896 | Steps: 167 | Reward: -101.8065 | Qmax: 5.3169\n",
      "| Episode: 897 | Steps: 244 | Reward: -106.5729 | Qmax: 5.5258\n",
      "| Episode: 898 | Steps: 268 | Reward: -107.2336 | Qmax: 5.4575\n",
      "| Episode: 899 | Steps: 384 | Reward: -119.4997 | Qmax: 5.4177\n",
      "| Episode: 900 | Steps: 204 | Reward: -112.0393 | Qmax: 5.5683\n",
      "| Episode: 901 | Steps: 258 | Reward: -108.2535 | Qmax: 5.4978\n",
      "| Episode: 902 | Steps: 1601 | Reward: -144.1061 | Qmax: 5.3726\n",
      "| Episode: 903 | Steps: 152 | Reward: -98.0300 | Qmax: 5.2295\n",
      "| Episode: 904 | Steps: 1601 | Reward: -174.0721 | Qmax: 5.3197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Episode: 905 | Steps: 135 | Reward: -98.0084 | Qmax: 5.3410\n",
      "| Episode: 906 | Steps: 177 | Reward: -99.8729 | Qmax: 5.2493\n",
      "| Episode: 907 | Steps: 483 | Reward: -114.1981 | Qmax: 5.2397\n",
      "| Episode: 908 | Steps: 265 | Reward: -106.1883 | Qmax: 5.1962\n",
      "| Episode: 909 | Steps: 210 | Reward: -102.5993 | Qmax: 5.1498\n",
      "| Episode: 910 | Steps: 285 | Reward: -120.0531 | Qmax: 5.2503\n",
      "| Episode: 911 | Steps: 229 | Reward: -118.5243 | Qmax: 5.2950\n",
      "| Episode: 912 | Steps: 104 | Reward: -100.2053 | Qmax: 5.2916\n",
      "| Episode: 913 | Steps: 115 | Reward: -103.2021 | Qmax: 5.1460\n",
      "| Episode: 914 | Steps: 190 | Reward: -112.1906 | Qmax: 5.1700\n",
      "| Episode: 915 | Steps: 208 | Reward: -103.9710 | Qmax: 5.2459\n",
      "| Episode: 916 | Steps: 172 | Reward: -104.9522 | Qmax: 5.1285\n",
      "| Episode: 917 | Steps: 245 | Reward: -102.9349 | Qmax: 5.2187\n",
      "| Episode: 918 | Steps: 216 | Reward: -105.6938 | Qmax: 5.1773\n",
      "| Episode: 919 | Steps: 369 | Reward: -111.0598 | Qmax: 5.1013\n",
      "| Episode: 920 | Steps: 234 | Reward: -108.0765 | Qmax: 5.1617\n",
      "| Episode: 921 | Steps: 351 | Reward: -108.3060 | Qmax: 5.1181\n",
      "| Episode: 922 | Steps: 307 | Reward: -114.2852 | Qmax: 5.1711\n",
      "| Episode: 923 | Steps: 421 | Reward: -139.4041 | Qmax: 5.1671\n",
      "| Episode: 924 | Steps: 214 | Reward: -106.2892 | Qmax: 5.0204\n",
      "| Episode: 925 | Steps: 304 | Reward: -110.4514 | Qmax: 4.8256\n",
      "| Episode: 926 | Steps: 352 | Reward: -124.0993 | Qmax: 4.9352\n",
      "| Episode: 927 | Steps: 196 | Reward: -104.4435 | Qmax: 4.8862\n",
      "| Episode: 928 | Steps: 166 | Reward: -105.8640 | Qmax: 5.0124\n",
      "| Episode: 929 | Steps: 146 | Reward: -97.7070 | Qmax: 4.9691\n",
      "| Episode: 930 | Steps: 221 | Reward: -113.0495 | Qmax: 5.2550\n",
      "| Episode: 931 | Steps: 473 | Reward: -139.7869 | Qmax: 4.8662\n",
      "| Episode: 932 | Steps: 959 | Reward: -169.8609 | Qmax: 5.1097\n",
      "| Episode: 933 | Steps: 588 | Reward: -136.7677 | Qmax: 4.9953\n",
      "| Episode: 934 | Steps: 636 | Reward: -148.7165 | Qmax: 5.0919\n",
      "| Episode: 935 | Steps: 1440 | Reward: -240.0525 | Qmax: 4.9580\n",
      "| Episode: 936 | Steps: 1381 | Reward: -213.5100 | Qmax: 4.9298\n",
      "| Episode: 937 | Steps: 501 | Reward: -130.5488 | Qmax: 4.8892\n",
      "| Episode: 938 | Steps: 363 | Reward: -113.9062 | Qmax: 4.8496\n",
      "| Episode: 939 | Steps: 783 | Reward: -190.4982 | Qmax: 4.9297\n",
      "| Episode: 940 | Steps: 180 | Reward: -100.4618 | Qmax: 4.8924\n",
      "| Episode: 941 | Steps: 180 | Reward: -101.7474 | Qmax: 4.8146\n",
      "| Episode: 942 | Steps: 108 | Reward: -97.6856 | Qmax: 4.9080\n",
      "| Episode: 943 | Steps: 149 | Reward: -100.6652 | Qmax: 4.7475\n",
      "| Episode: 944 | Steps: 166 | Reward: -103.7677 | Qmax: 5.0285\n",
      "| Episode: 945 | Steps: 277 | Reward: -120.1165 | Qmax: 4.8320\n",
      "| Episode: 946 | Steps: 218 | Reward: -110.9960 | Qmax: 4.9516\n",
      "| Episode: 947 | Steps: 175 | Reward: -104.2824 | Qmax: 4.8470\n",
      "| Episode: 948 | Steps: 1601 | Reward: -136.8455 | Qmax: 4.8139\n",
      "| Episode: 949 | Steps: 180 | Reward: -109.9612 | Qmax: 4.9414\n",
      "| Episode: 950 | Steps: 1601 | Reward: -146.1120 | Qmax: 4.7877\n",
      "| Episode: 951 | Steps: 196 | Reward: -99.0598 | Qmax: 4.7982\n",
      "| Episode: 952 | Steps: 142 | Reward: -105.5871 | Qmax: 4.8290\n",
      "| Episode: 953 | Steps: 315 | Reward: -109.9327 | Qmax: 4.9072\n",
      "| Episode: 954 | Steps: 159 | Reward: -99.7773 | Qmax: 4.9670\n",
      "| Episode: 955 | Steps: 163 | Reward: -101.3571 | Qmax: 4.6657\n",
      "| Episode: 956 | Steps: 122 | Reward: -99.4477 | Qmax: 4.6566\n",
      "| Episode: 957 | Steps: 189 | Reward: -107.9938 | Qmax: 4.8828\n",
      "| Episode: 958 | Steps: 372 | Reward: -108.5941 | Qmax: 4.9079\n",
      "| Episode: 959 | Steps: 1046 | Reward: -157.1097 | Qmax: 4.8328\n",
      "| Episode: 960 | Steps: 739 | Reward: -125.6012 | Qmax: 4.8467\n",
      "| Episode: 961 | Steps: 471 | Reward: -116.5086 | Qmax: 4.8188\n",
      "| Episode: 962 | Steps: 667 | Reward: -122.3101 | Qmax: 4.7642\n",
      "| Episode: 963 | Steps: 1336 | Reward: -162.5923 | Qmax: 4.7808\n",
      "| Episode: 964 | Steps: 1601 | Reward: -135.4071 | Qmax: 4.8620\n",
      "| Episode: 965 | Steps: 430 | Reward: -119.9730 | Qmax: 4.9320\n",
      "| Episode: 966 | Steps: 1601 | Reward: -114.7520 | Qmax: 4.8524\n",
      "| Episode: 967 | Steps: 1601 | Reward: -157.0298 | Qmax: 4.8484\n",
      "| Episode: 968 | Steps: 1601 | Reward: -135.6438 | Qmax: 4.7927\n",
      "| Episode: 969 | Steps: 256 | Reward: -106.3343 | Qmax: 4.9061\n",
      "| Episode: 970 | Steps: 158 | Reward: -111.0552 | Qmax: 4.7552\n",
      "| Episode: 971 | Steps: 1466 | Reward: -263.5378 | Qmax: 4.7795\n",
      "| Episode: 972 | Steps: 576 | Reward: -173.2227 | Qmax: 4.7585\n",
      "| Episode: 973 | Steps: 175 | Reward: -112.3552 | Qmax: 4.8300\n",
      "| Episode: 974 | Steps: 557 | Reward: -172.8262 | Qmax: 4.7764\n",
      "| Episode: 975 | Steps: 152 | Reward: -109.0771 | Qmax: 4.7662\n",
      "| Episode: 976 | Steps: 1601 | Reward: -146.4831 | Qmax: 4.7891\n",
      "| Episode: 977 | Steps: 59 | Reward: -111.3589 | Qmax: 4.8928\n",
      "| Episode: 978 | Steps: 280 | Reward: -106.6997 | Qmax: 4.7938\n",
      "| Episode: 979 | Steps: 1601 | Reward: -145.6831 | Qmax: 4.8285\n",
      "| Episode: 980 | Steps: 231 | Reward: -106.4981 | Qmax: 4.8829\n",
      "| Episode: 981 | Steps: 1601 | Reward: -128.3370 | Qmax: 4.8706\n",
      "| Episode: 982 | Steps: 1601 | Reward: -124.1806 | Qmax: 4.9863\n",
      "| Episode: 983 | Steps: 874 | Reward: -129.3404 | Qmax: 4.9957\n",
      "| Episode: 984 | Steps: 1601 | Reward: -112.5898 | Qmax: 5.1869\n",
      "| Episode: 985 | Steps: 1601 | Reward: -150.3852 | Qmax: 5.2714\n",
      "| Episode: 986 | Steps: 1601 | Reward: -139.5084 | Qmax: 5.3663\n",
      "| Episode: 987 | Steps: 227 | Reward: -98.2099 | Qmax: 5.3779\n",
      "| Episode: 988 | Steps: 308 | Reward: -114.4426 | Qmax: 5.4336\n",
      "| Episode: 989 | Steps: 1601 | Reward: -147.6853 | Qmax: 5.4348\n",
      "| Episode: 990 | Steps: 1601 | Reward: -125.3698 | Qmax: 5.4663\n",
      "| Episode: 991 | Steps: 232 | Reward: -105.4133 | Qmax: 5.5253\n",
      "| Episode: 992 | Steps: 214 | Reward: -100.0159 | Qmax: 5.5620\n"
     ]
    }
   ],
   "source": [
    "args = {'actor_lr': 0.0001,\n",
    "        'buffer_size': 1000000,\n",
    "        'continuous_act_space_flag': True,\n",
    "        'critic_lr': 0.001,\n",
    "        'double_ddpg_flag': True,\n",
    "        'env': 'BipedalWalker-v2',\n",
    "        'epsilon_decay': 0.001,\n",
    "        'epsilon_max': 1.0,\n",
    "        'epsilon_min': 0.01,\n",
    "        'exploration_strategy': 'epsilon_greedy',\n",
    "        'gamma': 0.99,\n",
    "        'max_episodes': 5000,\n",
    "        'minibatch_size': 64,\n",
    "        'monitor_dir': './results/gym_ddpg',\n",
    "        'random_seed': 1234,\n",
    "        'record_video_every': 1,\n",
    "        'render_env_flag': False,\n",
    "        'summary_dir': '../results2/BipedalWalker-v2/double_ddpg_softcopy_epsilon_greedy_run_test_log3',\n",
    "        'target_hard_copy_flag': False,\n",
    "        'target_hard_copy_interval': 200,\n",
    "        'tau': 0.001,\n",
    "        'use_gym_monitor': False,\n",
    "        'use_gym_monitor_flag': False}\n",
    "\n",
    "if not os.path.exists(args['summary_dir']):\n",
    "    os.makedirs(args['summary_dir'])\n",
    "log_dir = os.path.join(args['summary_dir'], 'ddpg_running_log.log')\n",
    "logging.basicConfig(filename=log_dir,\n",
    "                    filemode='a',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s',\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "for key in args.keys():\n",
    "    logging.info('{0}: {1}'.format(key, args[key]))\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
