{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import tflearn\n",
    "import argparse\n",
    "import pprint as pp\n",
    "\n",
    "from replay_buffer import ReplayBuffer\n",
    "\n",
    "import logging\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU' or x.device_type=='CPU']\n",
    "tf_availale_devices = get_available_devices()\n",
    "print(tf_availale_devices)\n",
    "\n",
    "# ===========================\n",
    "#   Actor and Critic DNNs\n",
    "# ===========================\n",
    "\n",
    "class ActorNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state, output is the action\n",
    "    under a deterministic policy.\n",
    "\n",
    "    The output layer activation is a tanh to keep the action\n",
    "    between -action_bound and action_bound\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, continuous_act_space_flag, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n",
    "        self.sess = sess\n",
    "        self.continuous_act_space_flag = continuous_act_space_flag\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Actor Network\n",
    "        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[\n",
    "            len(self.network_params):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n",
    "                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # This gradient will be provided by the critic network\n",
    "        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n",
    "\n",
    "        # Combine the gradients here\n",
    "        self.unnormalized_actor_gradients = tf.gradients(\n",
    "            self.scaled_out, self.network_params, -self.action_gradient)\n",
    "        self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n",
    "\n",
    "        # Optimization Op\n",
    "        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n",
    "            apply_gradients(zip(self.actor_gradients, self.network_params))\n",
    "\n",
    "        self.num_trainable_vars = len(\n",
    "            self.network_params) + len(self.target_network_params)\n",
    "\n",
    "    def create_actor_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        net = tflearn.fully_connected(inputs, 400)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        net = tflearn.fully_connected(net, 300)\n",
    "        net = tflearn.layers.normalization.batch_normalization(net)\n",
    "        net = tflearn.activations.relu(net)\n",
    "        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        # If Actor acts on discrete action space, use Softmax\n",
    "        if self.continuous_act_space_flag is True:\n",
    "            out = tflearn.fully_connected(\n",
    "                net, self.a_dim, activation='tanh', weights_init=w_init)\n",
    "        else:\n",
    "            out = tflearn.fully_connected(\n",
    "                net, self.a_dim, activation='softmax', weights_init=w_init)\n",
    "        # Scale output to -action_bound to action_bound\n",
    "        scaled_out = tf.multiply(out, self.action_bound)\n",
    "        return inputs, out, scaled_out\n",
    "\n",
    "    def train(self, inputs, a_gradient):\n",
    "        self.sess.run(self.optimize, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action_gradient: a_gradient\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        return self.sess.run(self.scaled_out, feed_dict={\n",
    "            self.inputs: inputs\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs):\n",
    "        return self.sess.run(self.target_scaled_out, feed_dict={\n",
    "            self.target_inputs: inputs\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "\n",
    "    def get_num_trainable_vars(self):\n",
    "        return self.num_trainable_vars\n",
    "\n",
    "\n",
    "class CriticNetwork(object):\n",
    "    \"\"\"\n",
    "    Input to the network is the state and action, output is Q(s,a).\n",
    "    The action must be obtained from the output of the Actor network.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n",
    "        self.sess = sess\n",
    "        self.s_dim = state_dim\n",
    "        self.a_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Create the critic network\n",
    "        self.inputs, self.action, self.out = self.create_critic_network()\n",
    "\n",
    "        self.network_params = tf.trainable_variables()[num_actor_vars:]\n",
    "\n",
    "        # Target Network\n",
    "        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n",
    "\n",
    "        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n",
    "\n",
    "        # Op for periodically updating target network with online network\n",
    "        # weights with regularization\n",
    "        self.update_target_network_params = \\\n",
    "            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n",
    "            + tf.multiply(self.target_network_params[i], 1. - self.tau))\n",
    "                for i in range(len(self.target_network_params))]\n",
    "\n",
    "        # Network target (y_i)\n",
    "        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "        # Define loss and optimization Op\n",
    "        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n",
    "        self.optimize = tf.train.AdamOptimizer(\n",
    "            self.learning_rate).minimize(self.loss)\n",
    "\n",
    "        # Get the gradient of the net w.r.t. the action.\n",
    "        # For each action in the minibatch (i.e., for each x in xs),\n",
    "        # this will sum up the gradients of each critic output in the minibatch\n",
    "        # w.r.t. that action. Each output is independent of all\n",
    "        # actions except for one.\n",
    "        self.action_grads = tf.gradients(self.out, self.action)\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        inputs = tflearn.input_data(shape=[None, self.s_dim])\n",
    "        action = tflearn.input_data(shape=[None, self.a_dim])\n",
    "        # TODO\n",
    "        powerful_critic = True\n",
    "        if powerful_critic is True:\n",
    "            net = tflearn.fully_connected(inputs, 400)\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "            net = tflearn.activations.relu(net)\n",
    "\n",
    "            # Add the action tensor in the 2nd hidden layer\n",
    "            # Use two temp layers to get the corresponding weights and biases\n",
    "            t1 = tflearn.fully_connected(net, 300)\n",
    "            t2 = tflearn.fully_connected(action, 300)\n",
    "\n",
    "            net = tflearn.activation(\n",
    "                tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "        else:\n",
    "            net = tflearn.fully_connected(inputs, 400)\n",
    "            net = tflearn.layers.normalization.batch_normalization(net)\n",
    "            net = tflearn.activations.relu(net)\n",
    "\n",
    "            # Add the action tensor in the 2nd hidden layer\n",
    "            # Use two temp layers to get the corresponding weights and biases\n",
    "            t1 = tflearn.fully_connected(net, 600)\n",
    "            t2 = tflearn.fully_connected(action, 600)\n",
    "\n",
    "            net = tflearn.activation(\n",
    "                tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "            # # Add the action tensor in the 2nd hidden layer\n",
    "            # # Use two temp layers to get the corresponding weights and biases\n",
    "            # t1 = tflearn.fully_connected(inputs, 300)\n",
    "            # t2 = tflearn.fully_connected(action, 300)\n",
    "            #\n",
    "            # net = tflearn.activation(\n",
    "            #     tf.matmul(inputs, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu')\n",
    "\n",
    "        # linear layer connected to 1 output representing Q(s,a)\n",
    "        # Weights are init to Uniform[-3e-3, 3e-3]\n",
    "        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n",
    "        out = tflearn.fully_connected(net, 1, weights_init=w_init)\n",
    "        return inputs, action, out\n",
    "\n",
    "    def train(self, inputs, action, predicted_q_value):\n",
    "        return self.sess.run([self.out, self.optimize], feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action,\n",
    "            self.predicted_q_value: predicted_q_value\n",
    "        })\n",
    "\n",
    "    def predict(self, inputs, action):\n",
    "        return self.sess.run(self.out, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: action\n",
    "        })\n",
    "\n",
    "    def predict_target(self, inputs, action):\n",
    "        return self.sess.run(self.target_out, feed_dict={\n",
    "            self.target_inputs: inputs,\n",
    "            self.target_action: action\n",
    "        })\n",
    "\n",
    "    def action_gradients(self, inputs, actions):\n",
    "        return self.sess.run(self.action_grads, feed_dict={\n",
    "            self.inputs: inputs,\n",
    "            self.action: actions\n",
    "        })\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.sess.run(self.update_target_network_params)\n",
    "        \n",
    "\n",
    "# Taken from https://github.com/openai/baselines/blob/master/baselines/ddpg/noise.py, which is\n",
    "# based on http://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "class OrnsteinUhlenbeckActionNoise:\n",
    "    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'OrnsteinUhlenbeckActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n",
    "\n",
    "# ===========================\n",
    "#   Tensorflow Summary Ops\n",
    "# ===========================\n",
    "\n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "# ===========================\n",
    "#   Agent Training\n",
    "# ===========================\n",
    "\n",
    "def train(sess, env, args, actor, critic, actor_noise):\n",
    "\n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(args['summary_dir'], sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Epsilon parameter\n",
    "    epsilon = args['epsilon_max']\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n",
    "\n",
    "    # Time step\n",
    "    time_step = 0.\n",
    "\n",
    "    # Needed to enable BatchNorm. \n",
    "    # This hurts the performance on Pendulum but could be useful\n",
    "    # in other environments.\n",
    "    # tflearn.is_training(True)\n",
    "\n",
    "    for i in range(int(args['max_episodes'])):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "        ep_steps = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            if args['render_env_flag']:\n",
    "                env.render()\n",
    "\n",
    "            # Added exploration noise\n",
    "            #a = actor.predict(np.reshape(s, (1, 3))) + (1. / (1. + i))\n",
    "            # TODO: different exploration strategy\n",
    "            a = []\n",
    "            action = []\n",
    "            exploration_strategy = args['exploration_strategy']\n",
    "            if exploration_strategy == 'action_noise':\n",
    "                a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n",
    "                # Convert continuous action into discrete action\n",
    "                if args['continuous_act_space_flag'] is True:\n",
    "                    action = a[0]\n",
    "                else:\n",
    "                    action = np.argmax(a[0])\n",
    "            elif exploration_strategy == 'epsilon_greedy':\n",
    "                if np.random.rand() < epsilon:\n",
    "                    if args['continuous_act_space_flag'] is True:\n",
    "                        a = np.reshape(env.action_space.sample(), (1, actor.a_dim))\n",
    "                    else:\n",
    "                        a = np.random.uniform(0, 1, (1, actor.a_dim))\n",
    "                else:\n",
    "                    a = actor.predict(np.reshape(s, (1, actor.s_dim)))\n",
    "                # Convert continuous action into discrete action\n",
    "                if args['continuous_act_space_flag'] is True:\n",
    "                    action = a[0]\n",
    "                else:\n",
    "                    action = np.argmax(a[0])\n",
    "            else:\n",
    "                print('Please choose a proper exploration strategy!')\n",
    "\n",
    "            s2, r, terminal, info = env.step(action)\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "\n",
    "            # Reduce epsilon\n",
    "            time_step += 1.\n",
    "            epsilon = args['epsilon_min'] + (args['epsilon_max'] - args['epsilon_min']) * np.exp(-args['epsilon_decay'] * time_step)\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > int(args['minibatch_size']):\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = \\\n",
    "                    replay_buffer.sample_batch(int(args['minibatch_size']))\n",
    "\n",
    "                if args['double_ddpg_flag']:\n",
    "                    # Calculate targets: Double DDPG\n",
    "                    target_q = critic.predict_target(\n",
    "                            s2_batch, actor.predict(s2_batch))\n",
    "                else:\n",
    "                    # Calculate targets\n",
    "                    target_q = critic.predict_target(\n",
    "                            s2_batch, actor.predict_target(s2_batch))\n",
    "                \n",
    "                \n",
    "\n",
    "                y_i = []\n",
    "                for k in range(int(args['minibatch_size'])):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + critic.gamma * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = critic.train(\n",
    "                    s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                a_outs = actor.predict(s_batch)\n",
    "                grads = critic.action_gradients(s_batch, a_outs)\n",
    "                actor.train(s_batch, grads[0])\n",
    "                \n",
    "                # Update target networks\n",
    "                if args['target_hard_copy_flag']:\n",
    "                    if ep_steps % args['target_hard_copy_interval'] == 0:\n",
    "                        actor.update_target_network()\n",
    "                        critic.update_target_network()\n",
    "                else:\n",
    "                    actor.update_target_network()\n",
    "                    critic.update_target_network()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "            ep_steps += 1\n",
    "\n",
    "            # if terminal or reach maximum length\n",
    "            if terminal:\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float((ep_steps + 1))\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, i)\n",
    "                writer.flush()\n",
    "\n",
    "                ep_stats = '| Episode: {0} | Steps: {1} | Reward: {2:.4f} | Qmax: {3:.4f}'.format(i,\n",
    "                                                                                             (ep_steps + 1),\n",
    "                                                                                             ep_reward,\n",
    "                                                                                             (ep_ave_max_q / float(ep_steps+1)))\n",
    "                print(ep_stats)\n",
    "                logging.info(ep_stats)\n",
    "                break\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        env = gym.make(args['env'])\n",
    "\n",
    "        np.random.seed(int(args['random_seed']))\n",
    "        tf.set_random_seed(int(args['random_seed']))\n",
    "        env.seed(int(args['random_seed']))\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        # Set action_dim for continuous and discrete action space\n",
    "        if args['continuous_act_space_flag'] is True:\n",
    "            action_dim = env.action_space.shape[0]\n",
    "            action_bound = env.action_space.high\n",
    "            # Ensure action bound is symmetric\n",
    "            assert (env.action_space.high == -env.action_space.low).all()\n",
    "        else:\n",
    "            action_dim = env.action_space.n\n",
    "            # If discrete action, actor uses Softmax and action_bound is always 1\n",
    "            action_bound = 1\n",
    "\n",
    "        # Use hardcopy way to update target NNs.\n",
    "        if args['target_hard_copy_flag'] is True:\n",
    "            args['tau'] = 1.0\n",
    "\n",
    "        actor = ActorNetwork(sess, args['continuous_act_space_flag'],\n",
    "                             state_dim, action_dim, action_bound,\n",
    "                             float(args['actor_lr']), float(args['tau']),\n",
    "                             int(args['minibatch_size']))\n",
    "\n",
    "        critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                               float(args['critic_lr']), float(args['tau']),\n",
    "                               float(args['gamma']),\n",
    "                               actor.get_num_trainable_vars())\n",
    "        \n",
    "        actor_noise = OrnsteinUhlenbeckActionNoise(mu=np.zeros(action_dim))\n",
    "\n",
    "        # Record videos\n",
    "        # Use the gym env Monitor wrapper\n",
    "        if args['use_gym_monitor_flag']:\n",
    "            monitor_dir = os.path.join(args['summary_dir'], 'gym_monitor')\n",
    "            env = wrappers.Monitor(env, monitor_dir,\n",
    "                                   resume=True,\n",
    "                                   video_callable=lambda count: count % args['record_video_every'] == 0)\n",
    "\n",
    "        train(sess, env, args, actor, critic, actor_noise)\n",
    "\n",
    "        if args['use_gym_monitor_flag']:\n",
    "            env.monitor.close()\n",
    "        else:\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='provide arguments for DDPG agent')\n",
    "\n",
    "#     # agent parameters\n",
    "#     parser.add_argument('--actor-lr', type=float, default=0.0001, help='actor network learning rate')\n",
    "#     parser.add_argument('--critic-lr', type=float, default=0.001, help='critic network learning rate')\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99, help='discount factor for critic updates')\n",
    "#     parser.add_argument('--tau', type=float, default=0.001, help='soft target update parameter')\n",
    "#     parser.add_argument('--buffer-size', type=int, default=1000000, help='max size of the replay buffer')\n",
    "#     parser.add_argument('--minibatch-size', type=int, default=64, help='size of minibatch for minibatch-SGD')\n",
    "#     parser.add_argument(\"--continuous-act-space-flag\", action=\"store_true\", help='act on continuous action space')\n",
    "\n",
    "#     parser.add_argument(\"--exploration-strategy\", type=str, choices=[\"action_noise\", \"epsilon_greedy\"],\n",
    "#                         default='epsilon_greedy', help='action_noise or epsilon_greedy')\n",
    "#     parser.add_argument(\"--epsilon-max\", type=float, default=1.0, help='maximum of epsilon')\n",
    "#     parser.add_argument(\"--epsilon-min\", type=float, default=.01, help='minimum of epsilon')\n",
    "#     parser.add_argument(\"--epsilon-decay\", type=float, default=.001, help='epsilon decay')\n",
    "\n",
    "#     # train parameters\n",
    "#     parser.add_argument('--double-ddpg-flag', action=\"store_true\", help='True, if run double-ddpg-flag. Otherwise, False.')\n",
    "#     parser.add_argument('--target-hard-copy-flag', action=\"store_true\", help='Target network update method: hard copy')\n",
    "#     parser.add_argument('--target-hard-copy-interval', type=int, default=200, help='Target network update hard copy interval')\n",
    "\n",
    "#     # run parameters\n",
    "#     # HalfCheetah-v2, Ant-v2, InvertedPendulum-v2, Pendulum-v0\n",
    "#     parser.add_argument('--env', type=str, default='HalfCheetah-v2', help='choose the gym env- tested on {Pendulum-v0}')\n",
    "#     parser.add_argument('--random-seed', type=int, default=1234, help='random seed for repeatability')\n",
    "#     parser.add_argument('--max-episodes', type=int, default=50000, help='max num of episodes to do while training')\n",
    "#     # parser.add_argument(\"--max-episode-len\", type=int, default=1000, help='max length of 1 episode')\n",
    "#     parser.add_argument(\"--render-env-flag\", action=\"store_true\", help='render environment')\n",
    "#     parser.add_argument(\"--use-gym-monitor-flag\", action=\"store_true\", help='record gym results')\n",
    "#     parser.add_argument(\"--record-video-every\", type=int, default=1, help='record video every xx episodes')\n",
    "#     parser.add_argument(\"--monitor-dir\", type=str, default='./results/gym_ddpg', help='directory for storing gym results')\n",
    "#     parser.add_argument(\"--summary-dir\", type=str, default='./results/tf_ddpg/HalfCheetah-v2/ddpg_Tau_0.001_run1', help='directory for storing tensorboard info')\n",
    "\n",
    "\n",
    "#     parser.set_defaults(use_gym_monitor=False)\n",
    "\n",
    "#     # args = vars(parser.parse_args())\n",
    "#     # args = parser.parse_args()\n",
    "#     args = vars(parser.parse_args())\n",
    "\n",
    "#     pp.pprint(args)\n",
    "\n",
    "#     if not os.path.exists(args['summary_dir']):\n",
    "#         os.makedirs(args['summary_dir'])\n",
    "#     log_dir = os.path.join(args['summary_dir'], 'ddpg_running_log.log')\n",
    "#     logging.basicConfig(filename=log_dir, filemode='a', level=logging.INFO)\n",
    "#     for key in args.keys():\n",
    "#         logging.info('{0}: {1}'.format(key, args[key]))\n",
    "\n",
    "#     main(args)\n",
    "\n",
    "#     # python ddpg_discrete_action.py --env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'actor_lr': 0.0001,\n",
    "        'buffer_size': 1000000,\n",
    "        'continuous_act_space_flag': True,\n",
    "        'critic_lr': 0.001,\n",
    "        'double_ddpg_flag': True,\n",
    "        'env': 'Ant-v2',\n",
    "        'epsilon_decay': 0.001,\n",
    "        'epsilon_max': 1.0,\n",
    "        'epsilon_min': 0.01,\n",
    "        'exploration_strategy': 'epsilon_greedy',\n",
    "        'gamma': 0.99,\n",
    "        'max_episodes': 5000,\n",
    "        'minibatch_size': 64,\n",
    "        'monitor_dir': './results/gym_ddpg',\n",
    "        'random_seed': 1234,\n",
    "        'record_video_every': 1,\n",
    "        'render_env_flag': False,\n",
    "        'summary_dir': '../results2/BipedalWalker-v2/double_ddpg_softcopy_epsilon_greedy_run_test_log3',\n",
    "        'target_hard_copy_flag': False,\n",
    "        'target_hard_copy_interval': 200,\n",
    "        'tau': 0.001,\n",
    "        'use_gym_monitor': False,\n",
    "        'use_gym_monitor_flag': False}\n",
    "\n",
    "if not os.path.exists(args['summary_dir']):\n",
    "    os.makedirs(args['summary_dir'])\n",
    "log_dir = os.path.join(args['summary_dir'], 'ddpg_running_log.log')\n",
    "logging.basicConfig(filename=log_dir,\n",
    "                    filemode='a',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s',\n",
    "                    datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "for key in args.keys():\n",
    "    logging.info('{0}: {1}'.format(key, args[key]))\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Ant-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.3\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/lingheng/tf_gpu/bin/python'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
